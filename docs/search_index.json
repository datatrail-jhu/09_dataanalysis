[["index.html", "09: Data Analysis About this Course", " 09: Data Analysis March, 2023 About this Course This course is part of a series of courses for DataTrail. DataTrail is a no-cost, paid 14-week educational initiative for young-adult, high school and GED-graduates. DataTrail aims to equip members of underserved communities with the necessary skills and support required to work in the booming field of data science. DataTrail is a fresh take on workforce development that focuses on training both Black, Indigenous, and other people of color (BIPOC) interested in the data science industry and their potential employers. Offered by the Johns Hopkins Bloomberg School of Public Health, in partnership with local non-profits and Leanpub, DataTrail combines a mutually-intensive learning experience (MILE) with a whole-person ecosystem of support to allow aspiring data scientists and their employers to succeed. DataTrail uses mutually-intensive learning DataTrail joins aspiring data science scholars and expert-level data scientist mentors in a mutually-intensive learning experience (MILE). In the DataTrail MILE: Scholars engage in cutting-edge technical and soft skills training needed to enter the data science field. Mentors engage in anti-racism and mentorship training needed to be impactful mentors and informed colleagues on diverse data science teams. The social connections created along the way will fuel job opportunities for scholars and foster a more diverse, equitable, and inclusive climate at the mentors’ institutions. "],["the-purpose-of-data-science.html", "Chapter 1 The Purpose of Data Science", " Chapter 1 The Purpose of Data Science Data science has multiple definitions. For this course we use the definition: Data science is the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience. In general the data science process is iterative and the different components blend together a little bit. But for simplicity lets discretize the tasks into the following 7 steps: Define the question you want to ask the data Get the data Clean the data Explore the data Fit statistical models Communicate the results Make your analysis reproducible This course is focused on three of these steps: (1) defining the question you want to ask, (4) exploring the data and (5) fitting statistical models to the data. We have seen in previous courses how to extract data from the web and from databases and we have seen how to clean it up and tidy the data. You also know how to use plots and graphs to visualize your data. You can think of this class as using those tools to start to answer questions using the tools you have learned in previous classes. 1.0.1 Types of data science questions We will look at a few different types of questions that you might want to answer from data. This flowchart gives some questions you can ask to figure out what type of question your analysis focuses on. Each type of question has different goals. There are four classes of question that we will focus on in this class. Descriptive: The goal of descriptive data science questions is to understand the components of a data set, describe what they are, and explain that description to others who might want to understand the data. This is the simplest type of data analysis. Exploratory: The goal of exploratory data science questions is to find unknown relationships between the different variables you have measured in your data set. Exploratory analysis is open ended and designed to find expected or unexpected relationships between different measurements. Inferential: The goal of inferential data science questions is to is to use a small sample of data to say something about what would happen if we collected more data. Inferential questions come up because we want to understand the relationships between different variables but it is too expensive or difficult to collect data on every person or object. Predictive: The goal of predictive data science question is to use data from a large collection to predict values for new individuals. This might be predicting what will happen in the future or predicting characteristics that are difficult to measure. Predictive data science is sometimes called machine learning. Types of Data Analysis The questions we will focus on are the types where we look for relationships between measurements or variables. But in these types of analyses we won’t be able to tell anything about what happens if you change one of the variables. To figure out what happens if you change a variable, you need a more advanced type of analysis. These analyses - causal and mechanistic - require data on specific types of problem and collected in special ways. For this class, the primary thing we need to be aware of is that just because two variables are correlated with each other it doesn’t mean that changing one causes a change in the other. One way that people illustrate this idea is to look at data where two variables show a relationship, but are clearly not related to each other. For example, in a specific time range, the number of people who drown while falling into a pool is related to the number of films that Nicholas Cage appears in. These two variables are clearly unrelated to each other, but the data seems to show a relationship. We’ll discuss more later in this class Spurious Correlation 1.0.2 Examples of data science questions Let’s begin with some example data science projects, how they are asked, and how they are approached. In each example, we explain the problem as a data science question and guess what sort of analysis is fit for approaching the problem. 1.0.2.1 Example 1: Credit card fraud detection If you have a credit card, every time you charge something, the bank keeps a record of that charge. This can be useful for you when you want to keep track of your finances. But banks use the information for other purposes as well. If you lose your credit card and someone starts using it to buy things for themselves this is called credit card fraud. By collecting data from everyone, credit card companies are able to predict potential fraudulent transactions before consumers notice anything. So the question is: “Can we predict which credit card charges are fraudulent?”. But this is not a data science question yet. To ask this like a data scientist we might ask, “Can we use the time of the charge, the location of the charge, and the price of the charge to predict whether that charge is fraudulent or not?”. Since we are interested in predicting whether a charge is fraud or genuine, this will be a predictive analysis. Problem: Detecting whether credit card charges are fraudulent. Data science question: Can we use the time of the charge, the location of the charge, and the price of the charge to predict whether that charge is fraudulent or not? Type of analysis: Predictive analysis 1.0.2.2 Example 2: Analysis of YouTube comments Any text you can find on the web can be considered to be data. You can use this data to understand how people behave on the web. For example,when companies like YouTube want to understand whether their users are behaving nicely or badly, they might look at the comments they leave. So the question might be, “Are the comments on our platform mostly nice or mostly mean?” To write this in terms of a data science question you might ask, “Are the words that people use in their comments more frequently positive words (great, awesome, nice, useful) or negative words (bad, stupid, lame, awful)?” This is a question you could answer by collecting information on words and labeling them with whether they are nice or mean. This is an example of descriptive analysis since once we have the data, the analysis boils down to comparing the number of positive comments to negative comments. Problem: Understanding whether users are nice or mean on YouTube Data science question: Are the words that people use in their comments more frequently positive words (great, awesome, nice, useful) or negative words (bad, stupid, lame, awful)? Type of analysis: Descriptive analysis 1.0.2.3 Example 3: Sesame Street and kids’ brain development Sesame Street is a children’s entertainment and educational program. Scientists might be interested in whether watching Sesame Street helps brain development in kids. To turn this into a data science question we need to make it more specific and focused on data. So we might convert this to a data science question like, “Can we compare children who watch Sesame Street and those who don’t to see whose test scores are higher?” A complication is that it might be difficult and expensive to get parents to have their children participate in the study. So we might take a small sample of students and measure their test scores and find out whether they watch TV. Since we want to say something about all children, but have only measured data on a few, this would be an inferential analysis. Problem: Does Sesame Street affect kids brain development? Data science question: Is there a relationship between watching Sesame Street and test scores among children? Type of analysis: Inferential analysis 1.0.3 How to start a data analysis The way to start a data analysis project is to start with a question. As we saw in the above examples, the questions should not start with the data. They should start with a general question that you want to start to answer. This approach is called “problem forward, not solution backward”. After you have asked your general question, the next step is to turn it into a data science question. This usually involves making the question more concrete, identifying what type of question you are asking, and identifying the parts of the data that you will use to answer the question. We will cover how to convert general questions to data science questions in a future lesson. The next steps involve exploring and analyzing the data. The bulk of the rest of this course will focus on the tools you can use to analyze data and find summaries or relationships that answer the questions you ask when starting a data analysis. 1.0.4 Slides and Video Automated Videos Slides "],["translating-questions-to-data-science-questions.html", "Chapter 2 Translating Questions to Data Science Questions", " Chapter 2 Translating Questions to Data Science Questions As we discussed in the first lesson, the approach to data analysis that we prefer is “problem forward, not solution backward”. The main point of this approach is to start with the question that you want to ask before you look at the data or the analysis. In some cases the question that you want to answer will be a question driven by your curiosity. For example, you may be interested in a question about your fitness. You could collect data using a Fitbit and the MyFitnessPal app. Or you may have a question about what kind of songs you like best and collect data from your Spotify profile. You might also be interested in where the potholes are most common in your city. You could collect information from your city’s open data website. Another really common situation is that someone else is coming up with the question. When you are working as a data scientist this might be the marketing team, and executive, or an engineer who has a question that they would like to answer with the data. You might be asked to categorize photos on a website like Airbnb. They might bring you the question, the data or both. Part of your job as a data scientist is to translate general questions into data science questions. The first step in translating a general question into a data science question is to make it as concrete as possible. For example here are some generic questions you might be interested in: When I run more do I lose weight? Are customers more likely to click on ads with puppies? Do I need to take an umbrella with me when I leave the house today? These questions are interesting but they aren’t very specific. This is how most good data science projects start. To make a question more concrete you need to think about the data you would use to answer the question. This could either be data that you have or data that you think you could find. For each of these questions you need to ask these questions: What or who am I trying to understand with data? What measurements do I have on those people or objects that help me answer the question? How do the data I have limit the type of question I can answer? What is the type of data science question we are trying to answer? The first question is focused on figuring out who or what you are trying to study. In the world of statistics this is sometimes called the “population” you are trying to study. When you ask a question it is best to be as specific as possible about what you are trying to study. A good way to be specific is to imagine the individual people or objects you are going to measure data on. Realistically, what is the group that you have or will collect data to measure? The second question focuses on figuring out which variables are measured or will be measured in the data that you have. We have discussed previously about all the different potential data types you might have, including standard quantitative or qualitative data, text, images, or videos can be data. When answering this question it helps to be specific. For example, unstructured text from a social media post may not be helpful, but words and labels for the words in that post may be the data that you are looking for. The third question is critical for being careful in a data analysis. When you use the problem forward approach, you might start with a general question. But it might not be possible to answer that question with data we have. For example, it may be difficult to study directly the way that cigarette smoke affects children since most children don’t smoke. You might have to change your question to studying the way that second-hand smoking affects children or the way that parents smoking habits affect children. A key part of translating a general question into a data science question is identifying these limitations. The fourth question is focused on figuring out what type of analysis you are doing. We introduced the flowchart for defining the question type in an earlier lesson. The key questions to ask are how the data are summarized, what are the goals you are trying to achieve, and what does success for your analysis look like? One of the most common errors that people make in doing a data analysis is to answer the wrong type of data analytic question. Let’s try this approach out on a couple of made up examples and a real example to help you understand how to translate general questions into data science questions. When I run more do I lose weight? What or who am I trying to understand with data? I’m trying to understand something about only myself, not about others. What measurements do I have on those people or objects that help me answer the question? I have data on how many steps I take with Fitbit and measure my weight with a scale. How do the data I have limit the type of question I can answer? I only have data on me. I only have measurements on my weight every day and I need to summarize my Fitbit data to understand my runs, but won’t have information on whether I ran up and down hills or any information on my diet. What is the type of data science question we are trying to answer? In this case we are looking for a relationship between two variables that we measured for only the data we have so it is an exploratory analysis. When I run more do I lose weight? Are customers more likely to click on ads with puppies? What or who am I trying to understand with data? I’m trying to understand something about customers. I would need to figure out which customers. Customers trying to buy motorbikes might be different than customers going to Pets.com. We’d have to ask further questions to figure out which customers we are talking about. What measurements do I have on those people or objects that help me answer the question? Suppose we have all the data from a set of customers who visited a website for buying dog food for a single day. Some of the customers saw a puppy ad and some didn’t. We also have data on how much dog food they bought. How do the data I have limit the type of question I can answer? I only have data on a single website and only on a single day. So I might not be able to say things about other websites or other days. What is the type of data science question we are trying to answer? In this case we are looking for a relationship between two variables and trying say something about all the customers for a website. So this is an inferential analysis. Are customers more likely to click on ads with puppies? Do I need to take an umbrella with me when I leave the house today? What or who am I trying to understand with data? I’m trying to predict the weather in my hometown on a particular day so I know whether to take an umbrella. What measurements do I have on those people or objects that help me answer the question? I could take predictions from different weather services, look out the window and see if it is cloudy, or go out and feel if it is humid outside. To build my prediction I could collect these measurements for a year and also record whether I needed an umbrella that day. How do the data I have limit the type of question I can answer? I only have data on my hometown, I’ve only collected data from a few weather prediction services, and the data are only collected over one year. So it might be hard to say things for other people, other places, or other times. What is the type of data science question we are trying to answer? In this case we are looking to use historical data to predict something about a single day. So this is a prediction problem. Do I need to take an umbrella with me when I leave the house today? 2.0.1 A real example Let’s practice translating questions to data science questions through an example. We briefly mentioned this example in the course Getting Data. The analysis is based on data scientist David Robinson’s blog. Trump Tweets blog post A Twitter user came up with a hypothesis that when Donald Trump was tweeting, hyperbolic tweets came from an Android phone (which he suggested were coming from Donald Trump) and non-hyperbolic tweets came from an iPhone (which he suggested came from Donald Trump’s staff). This twitter had noticed that “When Trump wishes the Olympic team good luck, he’s tweeting from his iPhone. When he’s insulting a rival, he’s usually tweeting from an Android.” Robinson started with this question and translated it into a data science question he could answer with data he could collect from Twitter. What or who am I trying to understand with data? In this case we are trying to answer a question about who is tweeting from the account RealDonaldTrump on Twitter. Specifically, we are interested in the differences between Tweets coming from iPhones versus those coming from Android phones. What measurements do I have on those people or objects that help me answer the question? Since we care about the differences between iPhones and Android phones we need to know which kind of phone each Tweet came from. But this information is available for each Tweet. We could collect this information by looking at the _(RealDonaldTrump_?) twitter profile or we could use the Twitter API to extract this data for each tweet. Robinson collects data on the type of phone for all the tweets from the account using the API. We also need to figure out how to define “hyperbolic” which was the hypothesis in the original question. It is hard to directly label tweets as hyperbolic or non-hyperbolic. But we can collect the text of the tweets themselves. If we alter the question a little to “Are the Android tweets angrier and more negative than the iPhone tweets?” we can use the text of the tweets themselves to answer this question. There are lists of words that are already labeled as “angry” so we can look for those words among the tweets from Android phones and iPhones. We can also collect the time that each tweet occurred, since this will help us separate tweets coming from one type of phone or another. How do the data I have limit the type of question I can answer? One critical piece of information we are missing from this data is who sent the tweet out. We can hypothesize that one person has an Android and a different person has an iPhone. But we can’t say for sure which person is tweeting from which type of phone. So we have changed the question from a question about who is tweeting to what is the difference between tweets from the two types of phones. What is the type of data science question we are trying to answer? Using some exploratory analysis, David Robinson first found that the most common words the come from the Android and iPhone platforms are different. He uses a measure to find the likelihood that a word is tweeted from an Android or an iPhone phone. So the words “badly” or “crazy” are likely to be sent from Android and the hashtags #makeamericagreatagain and #trump2016 are likely from iPhone. Exploratory Analysis Next he can label each word with one of 10 sentiments: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust using lists that have been put together in the tidytext package. Robinson found that Tweets that come from the Android account use about 40-80% more words related to disgust, sadness, fear, anger, and other “negative” sentiments than the iPhone account does. Sentiment Analysis This exploratory analysis suggested that there is a difference between tweets coming from an Android phone versus an iPhone. If you read the post, you will see that further analysis suggests that there are very different types of tweets happening at very different times. This doesn’t show that one person or another is sending those tweets which would require other sources of data we don’t have. 2.0.2 Slides and Video Automated Videos Slides "],["do-i-have-the-data-i-need.html", "Chapter 3 Do I Have The Data I Need?", " Chapter 3 Do I Have The Data I Need? After identifying your problem and transforming it into a data science question, your next step is to identify and ensure that you have access to the data necessary to answer the question. However, good data can be hard to come by. Depending on your question, it can be costly or simply infeasible to obtain the data you need. For instance, consider wanting to know the answer to the question “Does money make people happy?” One way to get data for your problem is to give people cash and to collect data from those same individuals later to see if the money they received made happier. First, this isn’t a cheap experiment, so you may not have the money to do this. Second, how would you want to measure happiness? You can’t just step on a scale and measure someone’s happiness. If you had each individual report their happiness on a scale of 1-10, one person may rate their happiness at an 8. Another person, who could be just as happy as the person who rated their happiness at an 8 may rate their happiness at a 6. You can see quickly how getting the right data to answer this question may not be so simple. Does money make people happy? In another situation, what if you are interested in answering the question “Does texting while driving causes accidents?” It wouldn’t be ethical to run a study and tell people to text while they’re driving and then determine if those who texted had more care accidents. It’s not okay to design experiments that knowingly put participants at risk of harm. Does texting while driving cause accidents? Thus, although each of us generate data daily through the apps we use or through our activity on social media platforms, often the data most easily available are not the data we need to answer the questions we’re most interested in answering. In this lesson we’ll discuss how to determine whether or not you have the data you need to answer the question you have, limitations of the data you have, considerations to make when you don’t yet have the data you need, and what to do to get the data you need. 3.0.1 The Data Science Question In the last lesson we discussed how to take a problem you’re interested in understanding better and turning it into a data science question. Specifically, the last lesson looked at David Robinson’s blog post analyzing Donald Trump’s tweets. David Robinson set out to answer the specific question **“How do the tweets on the _(realDonaldTrump_?) account differ between those posted using an iPhone vs. those posted using an Android?“** Before answering the question, David Robinson determined what/whose data would need to be collected, what measurements he would need, the limitations to the analysis, and the type of analysis he would use. In this lesson, we’re going to really focus on the second and third questions there. What data do you need to answer your data science question and what limitations do these data have? We’ll focus on the details of how to get the data necessary to answer the question of interest. We’ll walk through this process again using the Trump Tweets blog post as an example. Trump Tweets Blog Post 3.0.2 The Perfect Dataset: The Data We Want Once you determine your data science question of interest, it’s helpful to envision the perfect dataset. This dataset likely doesn’t exist in the real world, but our goal will be to get data that match this optimal dataset as closely as possible. Thus to answer the question Do the tweets on the _(realDonaldTrump_?) account differ between those posted using an iPhone vs. those posted using an Android? Are the Android tweets angrier and more negative? we would ideally want a dataset where each row is a different tweet with each row containing information on the following variables: date - date the tweet was written time - time the tweet went out os - the operating system of where the tweet came from (Android or iPhone) author- author of each tweet location - where the author was when the tweet was written tweet - what the tweet said censored - whether or not the tweet was later deleted censored_date - date on which the tweet was deleted censored_time - time at which the tweet was deleted anger - some measurement of how angry and negative the tweet is Additionally, in a perfect world, we’d have all the tweets ever tweeted from _(realDonaldTrump_?). Having all these data in this format would enable us to answer the question of interest! 3.0.3 The Data We Have But, the data we have are typically not exactly the data we want. In this case, David Robinson was able to use the twitteR package and the Twitter API to obtain a number of Donald Trump’s tweets. Ultimately, the dataset he used for his analysis contained 628 tweets from an iPhone, and 762 tweets from an Android. That’s not nearly all the tweets from this account; however, the Twitter API limits the number of tweets one can obtain. Already, the data we have are not the optimal data we wanted, but that’s OK. Additionally, the data from the Twitter API does not include the author, the author’s location, nor any information about deleted tweets. Again, the data we have are quite different from our optimal data. Finally, the dataset does not include any measure of how angry or negative the content of the tweets are. David will have to use the tweets to determine that during his analysis. The data we have Nevertheless, David did have information on this subset of tweets, including the date, time, os, and what the tweet said. While not exactly the data he would have likely liked to have, David was still able to use these data while noting the limitations of the data in his blog post! 3.0.4 The Data We Can Get (Easily) In this case, David Robinson had enough data to answer the question he was interested in. However, often you may not have the data you need right from the beginning. After looking at the data you have, it’s worth considering whether or not there are data that you need that you can easily get access to. Depending upon your data science question, you may consider obtaining data from any of the sources discussed in the Getting Data course. For example, you could look at data from: Government Data APIs Open Datasets (such as those on Kaggle or data.world) your company These are all examples of places where it’s usually pretty easy to obtain data. And, ideally, whenever possible, you’ll want to work with raw data – data that haven’t yet been cleaned (i.e. outliers have not yet been removed) so that you know you have the most complete dataset from the start. You can then clean and wrangle the data after getting the raw data. However, what if the data you want don’t exist in any of these places? When the data aren’t available, it’s up to you (your team, your boss, or your company) to collect the data. 3.0.5 Data Collection In practice, data are defined by how they are collected. While it may not always be up to you to determine how the data are going to be collected, it’s important to understand the different ways in which data are collected: Observational data are collected from a sample of the bigger population of interest. For instance, data on household expenditure in the U.S. are usually collected from a sample of American household. Observational data can be in the following forms: Cross-sectional data are collected from a sample of the bigger population of interest at a specific point in time. Longitudinal or panel data are collected from a sample of the population of interest but at multiple points in time. For instance, data on the effect of peer effect in school on life outcomes are measured at multiple points in the lives of the individuals. In longitudinal or panel data, the sample does not change from one point in time to another. For example, if data is collected on 1000 individuals in the year 2010, in 2015 the data is likely collected from the same 1000 individuals. Experimental data are collected through a randomized experiment. In an experiment, the researchers divide the sample into to or more (that is chosen from the bigger population of interest) and assign the treatment (let’s say the vaccination) to one group while the group receives no treatment. The outcomes are then observed and compared. This differs from observational data, which involves collecting data without changing any of the existing conditions. If you are able to generate the dataset you need or collect the data you wanted to answer your question, then you’re all set. But, what if there are limitations to you getting those data? 3.0.6 The Data We Can’t Get Often there will be data you would love to have to answer your data science question, but you won’t be able to get those data. There are a number of limitations to data collection. We’ll discuss a number of common ones here. 3.0.6.1 Limited Resources In the introduction to this lesson, we discussed an experiment where we would give people money and measure their happiness. However, if we didn’t have the money to carry out this experiment, we wouldn’t be able to run this experiment and thus, money would be a limited resource. However, money is not the only limiting factor. What if you need to know the answer to your question next month but it would take you a year to collect the necessary data? In this case time would be a limited resource. Alternatively, what if another company had the data you needed, but the company you work for is a competitor. In this case, they likely wouldn’t share the data with you. Access to data can also be limiting. 3.0.6.2 Ethical Limitations In the introduction to this lesson, we also talked about the case where we wanted to know if texting increases the number of accidents. We mentioned that we wouldn’t be able to carry out the experiment of telling people to text when they’re driving and then seeing if those people get into more accidents due to ethical limitations. It is unethical to collect data knowingly putting individuals in harms way. However, there are more nuanced situations as well. What if you want to collect data about reproductive health? You may want to know everything about each mother’s medical history, including very sensitive and personal information. For example, you may want to know whether or not each individual in your dataset has ever had an abortion. Asking sensitive and invasive questions should only be asked when absolutely necessary. Further, this data will always have to be stored securely, and can only be asked with approval of and oversight committee (such as an Institutional Review Board). Thus, while it may be helpful to have information about certain variables, it may be inappropriate or too invasive to obtain this information. 3.0.6.3 Security Mentioned briefly above, but to really spell it out here: not all data are publicly-available and not all data should be publicly-available. Personal data and data that contain sensitive information must be secured properly and cannot be shared freely. While the data you want may exist somewhere, you may not have access to it, nor will you be granted access to it for security reasons. In these cases, you will not be able to use these data. 3.0.7 Questions to Ask Ourselves If you run into problems obtaining or collecting the data you need to answer your question, there are a number of questions to ask yourself: Can I change the question in such a way that is still interesting but that can be answered with the data that I do have? If I use the data to which I have access, is the project still worth doing? Will the project be feasible if I change it? If it is still feasible, I’ll need to rework the question and redesign my data collection plan. What will that look like? In the case where you decide to change the question and determine that the project is still worth doing at this point, it’s important to go back through the exercise of determining what data you would optimally have, what you do have, and what you can get easily to ensure that you won’t be wasting your time by moving forward with the project. However, in some cases, you’ll have a super interesting question, but you won’t be able to do the project given the data you have. In these situations, save the idea for a later point when you may have the resources or access to the data you need, and then move onto the next one. Everyone has had to abandon projects in their career, and it’s OK to move on. It’s much better to leave a project behind in the planning stages than to spend months or years on a project/analysis that was doomed from the start! 3.0.8 Are the Data We Have Good Data? All that said, once you have your data science question and the data you want to use, you’ll need to determine whether the data are good enough. If they’re not, before you spend hours on it, quit and look for other sources of data. 3.0.8.1 The need to wrangle For example, data downloaded from other resources often need to be cleaned and wrangled. (Note: If the data that you obtain, however, are pre-processed, make sure you understand how the processing was done!) To clean your data, you’ll always want to record the steps you take to reformat the data. We suggest the following steps to check and tidy the data for your analysis. Make sure: each variable forms a column each observation forms a row each table/file stores data about one kind of observation (e.g. people/hospitals). if variables are collected from multiple sources, they are merged properly column names are easy to use and informative apparent mistakes in the data have been removed missing values are formatted uniformly and correctly variable values are internally consistent appropriate transformed variables have been added These are the concepts previously discussed in the Data Tidying lessons in an earlier course in this Course Set, so many of them should be familiar. Cleaning data to make sure the dataset is in a tidy format, that the variables are all appropriately-named, and that the values within each variable are as you expect them to be is an important step in determining whether or not the data you have will be useful in answering your data science question. These topics will be further discussed in the Descriptive Analysis and Exploratory Analysis lessons in the later lessons in this course. 3.0.8.2 When data aren’t good That all said, let’s assume you have the dataset that contains the variables you are looking for, and it is tidy and ready to go for your analysis. It’s always nice to step back to make sure the data is the right data before you spend hours and hours on your analysis. So, let’s discuss some of the potential and common issues people run into with their data. 3.0.8.2.1 Number of observations is too small It happens quite often that collecting data is expensive or not easy. For instance, in a medical study on the effect of a drug on patients with Alzheimer disease, researchers will be happy if they can get a sample of 100 people. These studies are expensive, and it’s hard to find volunteers who enroll in the study. It is also the case with most social experiments. While data are everywhere, the data you need may not be. Therefore, most data scientists at some point in their career face the curse of small sample size. Small sample size makes it hard to be confident about the results of your analysis. So when you can, and it’s feasible, a large sample is preferable to a small sample. But when your only available dataset to work with is small you will have to note that in your analysis. Although we won’t learn them in this course, there are particular methods for inferential analysis when sample size is small. 3.0.8.2.2 Dataset does not contain the exact variables you are looking for In data analysis, it is common that you don’t always have what you need. You may need to know individuals’ IQ, but all you have is their GPA. You may need to understand food expenditure, but you have total expenditure. You may need to know parental education, but all you have is the number of books the family owns. It is often that the variable that we need in the analysis does not exist in the dataset and we can’t measure it. In these cases, our best bet is to find the closest variables to that variable. Variables that may be different in nature but are highly correlated with (similar to) the variable of interest are what are often used in such cases. These variables are called proxy variables. For instance, if we don’t have parental education in our dataset, we can use the number of books the family has in their home as a proxy. Although the two variables are different, they are highly correlated (very similar), since more educated parents tend to have more books at home. So in most cases where you can’t have the variable you need in your analysis, you can replace it with a proxy. Again, it must always be noted clearly in your analysis why you used a proxy variable and what variable was used as your proxy. 3.0.8.2.3 Variables in the dataset are not collected in the same year Imagine we want to find the relationship between the effect of cab prices and the number of rides in New York City. We want to see how people react to price changes. We get a hold of data on cab prices in 2018, but we only have data on the number of rides from 2015. Can these two variables be used together in our analysis? Simply, no. If we want to answer this question, we can’t match these two sets of data. If we’re using the prices from 2018, we should find the number of rides from 2018 as well. Unfortunately, a lot of the time, this is an issue you’ll run into. You’ll either have to find a way to get the data from the same year or go back to the drawing board and ask a different question. This issue can be ignored only in cases where we’re confident the variables does not change much from year to year. 3.0.8.2.4 Dataset is not representative of the population that you are interested in You will hear the term representative sample, but what is it? Before defining a representative sample, let’s see what a population is in statistical terms. We have used the word population without really getting into its definition. A sample is part of a population. A population, in general, is every member of the whole group of people we are interested in. Sometimes it is possible to collect data for the entire population, like in the U.S. Census, but in most cases, we can’t. So we collect data on only a subset of the population. For example, if we are studying the effect of sugar consumption on diabetes, we can’t collect data on the entire population of the United States. Instead, we collect data on a sample of the population. Now, that we know what sample and population are, let’s go back to the definition of a representative sample. A representative sample is a sample that accurately reflects the larger population. For instance, if the population is every adult in the United States, the sample includes an appropriate share of men and women, racial groups, educational groups, age groups, geographical groups, and income groups. If the population is supposed to be every adult in the U.S., then you can’t collect data on just people in California, or just young people, or only men. This is the idea of a representative sample. It has to model the broader population in all major respects. We give you one example in politics. Most recent telephone poles in the United States have been bad at predicting election outcomes. Why? This is because by calling people’s landlines you can’t guarantee you will have a representative sample of the voting age population since younger people are not likely to have landlines. Therefore, most telephone polls are skewed toward older adults. Random sampling is a necessary approach to having a representative sample. Random sampling in data collection means that you randomly choose your subjects and don’t choose who gets to be in the sample and who doesn’t. In random sampling, you select your subjects from the population at random like based on a coin toss. The following are examples of lousy sampling: A research project on attitudes toward owning guns through a survey sent to subscribers of a gun-related magazine (gun magazine subscribers are not representative of the general population, and the sample is very biased) A research project on television program choices by looking at Facebook TV interests (not everybody has a Facebook account) A research study on school meals and educational outcomes done in a neighborhood with residents mainly from one racial group (school meal can have a different effect on different income and ethnic groups) A researcher polls people as they walk by on the street. A TV show host asks the program viewers to visit the network website and respond to a poll. With this logic, most online surveys or surveys on social media has to be taken with a grain of salt because not members of all social groups have an online presentation or use social media. The moral of the story is that always think about what your population is. Your population will change from one project to the next. If you are researching the effect of smoking on pregnant women, then your population is, well, pregnant women (and not men). After you know your population, then you will always want collect data from a sample that is representative of your population. Random sampling helps. And lastly, if you have no choice but to work with a dataset that is not collected randomly and is biased, be careful not to generalize your results to the entire population. If you collect data on pregnant women of age 18-24, you can’t generalize your results to older women. If you collect data from the political attitudes of residents of Washington, DC, you can’t say anything about the whole nation. 3.0.8.2.5 Some variables in the dataset are measured with error Another curse of a dataset is measurement error. In simple, measurement error refers to incorrect measurement of variables in your sample. Just like measuring things in the physical world comes with error (like measuring distance, exact temperature, BMI, etc.), measuring variables in the social context can come with an error. When you ask people how many books they have read in the past year, not everyone remembers it correctly. Similarly, you may have measurement error when you ask people about their income. A good researcher recognizes measurement error in the data before any analysis and takes it into account during their analysis. 3.0.8.2.6 Variables are confounded What if you were interested in determining what variables lead to increases in crime? To do so, you obtain data from a US city with lots of different variables and crime rates for a particular time period. You would then wrangle the data and at first you look at the relationship between popsicle sales and crime rates. You see that the more popsicles that are sold, the higher the crime rate. popsicles and crime rate Your first thought may be that popsicles lead to crimes being committed. However, there is a confounder that’s not being considered! We will see in detail what confounders are in the lesson on Inferential Analysis but in short confounders are other variables that may affect our outcome but are also correlated with (have a relationship with) our main variable of interest. In the popsicle example, temperature is an important confounder. More crimes happen when it’s warm out and more popsicles are sold. It’s not the popsicles at all driving the relationship. Instead temperature is likely the culprit. temperature is a confounder Therefore, if we’re really interested in what increases crime rate, we should also consider the temperature. Thus, it’s important to understand the relationship between the variables in your dataset. This will be further discussed in the Exploratory Analysis lesson. 3.0.9 A case study: Why were polls so wrong about the 2016 US election? The results of the 2016 US presidential election came as a surprise to a lot of the media outlets and poll experts. Most polls consistently projected that Hillary Clinton would defeat Donald Trump. Election forecasters put Clinton at a winning position at a chance as high as 70-99 percent. Polls are usually trustworthy, but what happened in 2016? Why were polls so wrong about the 2016 US election? We don’t know the whole story yet. But for starters, the one issue everyone agrees is the data used in the prediction algorithms were not wholly appropriate. To be more specific, the problem was what is called the nonresponse bias. Nonresponse bias is the bias the sample caused by a particular section of the population systematically refusing to answer to the poll or survey. The consequence of nonresponse bias is a sample that is not representative of the population as we discussed above. (For instance, most rich people refuse to announce their income in surveys and as a result, there is nonresponse bias in most surveys of income. Ok, back to the election. It is known that less educated voters who constituted a bulk of Trump voters are hard for pollsters to reach. The result of this was the lack of this pro-Trump segment of the population in the polls during the weeks leading to the election. Pew Research also points out to dishonesty in responses to polls (people not being truthful about who they were planning to vote for) as another reason why polls failed to predict the outcome of the election accurately. While it’s always easier to look back at a failed analysis and determine what went wrong then to identify the issues up front, researchers would have benefited from determining if the data they had were the most appropriate for answering their question of interest. 3.0.10 Summary Once you’ve honed in on a good data science question, it’s important to determine if the data available to you are the data you need. To do this: Imagine the optimal dataset Determine what data you have Identify the data you can (easily) get Figure out limitations in the data you have Are the limitations so great that you need to re-work your question? If yes, start over and form a new data science question, then return to step 1 If not, continue with your analysis, but note all limitations in your analysis Explore and wrangle the dataset Analyze the data and answer the data science question! All in all, the data and how they’re collected matters. There is a term in statistics that says “garbage in, garbage out,” which means poor quality input will always produce poor output. A data analysis that is based on faulty data produces faulty results. Be cautious about the data that you use in your analysis, ensure that the data you need to answer your question of interest are the data you have, and always tell your listeners and readers about the deficiencies of your data. After all, you will know your data better than anyone else! 3.0.11 Slides and Video Automated Videos Slides "],["descriptive-analysis.html", "Chapter 4 Descriptive Analysis", " Chapter 4 Descriptive Analysis The goal of descriptive analysis is to describe or summarize a set of data. Whenever you get a new dataset to examine, this is the first analysis you will perform. In fact, if you never summarize the data, it’s not a data analysis. Descriptive Analysis summarizes the dataset If you think of a dataset as the animal in the elephant in the middle of this picture, you doing a descriptive analysis are the blind monks examining every part of the elephant. Just like the blind monks who are examining each and every part of the elephant to understand it completely, the goal of a descriptive analysis is to understand the data you’re working with completely. examining a dataset Descriptive analysis will first and foremost generate simple summaries about the samples and their measurements to describe the data you’re working with. There are a number of common descriptive statistics that we’ll discuss in this lesson: measures of central tendency (eg: mean, median, mode) or measures of variability (eg: range, standard deviations or variance). This type of analysis is aimed at summarizing your dataset. Unlike analysis approaches we’ll discuss in coming lessons, descriptive analysis is not for generalizing the results of the analysis to a larger population nor trying to draw any conclusions. Description of data is separated from interpreting the data. Here, we’re just summarizing what we’re working with. Some examples of purely descriptive analysis can be seen in censuses. In a census, the government collects a series of measurements on all of the country’s citizens. After collecting these data, they are summarized. From this descriptive analysis, we learn a lot about a country. For example, you can learn the age distribution of the population by looking at U.S. census data. 2010 census Data broken down by age This can be further broken down (or stratified) by sex to describe the age distribution by sex. The goal of these analyses is to describe the population. No inferences are made about what this means nor are predictions made about how the data might trend in the future. The point of this (and every!) descriptive analysis is only to summarize the data collected. 2010 census Data broken down by age and sex In this lesson, we’ll discuss the steps required to carry out a descriptive analysis. As this will be the first thing you do whenever you’re working with a new dataset, it’s important to work through the examples in this lesson step-by-step. 4.0.1 How to Describe a Dataset When provided a new tabular dataset, whether it’s a CSV you’ve been sent by your boss or a table you’ve scraped from the Internet, the first thing you’ll want to do is describe the dataset. This helps you understand how big the dataset is, what information is contained in the dataset, and how each variable in the dataset is distributed. Descriptive Analysis For this lesson, we’re going to move away from the iris or mtcars dataset (since you probably already have a pretty good understanding of those datasets) and work with a dataset we haven’t used too much in this Course Set: msleep (from the ggplot2 package). This dataset includes information about mammals and their sleep habits. We’ll load that package in and assign the dataset to the object df: ## install and load package install.packages(&quot;ggplot2&quot;) library(ggplot2) ## assign to object `df` df &lt;- msleep Generally, the first thing you’ll want to know about your dataset is how many observations and how many variables you’re working with. You’ll want to understand the size of your dataset. You can always look at the Environment tab in RStudio Cloud to see how many observations and variables there are in your dataset; however, once you have many objects, you’ll have to scroll through or search this list to find the information you’re looking for. Environment tab To avoid having to do that, the simplest approach to getting this information is the dim() function, which will give you the dimensions of your data frame. The output will display with the number of rows (observations) first, followed by the number of columns (variables). ## determine the dimensions dim(df) dim() output 4.0.1.1 Variables There are additional ways to learn a bit more about the dataset using a different function. What if we wanted to know not only the dimensions of our dataset but wanted to learn a little bit more about the variables we’re working with? Well, we could use dim() and then use the colnames() function to determine what the variable names are in our dataset: ## determine variable names colnames(df) The output from colnames tells us that there are 11 variables in our dataset and lets us know what the variable names are for these data. colnames() output But, what if we didn’t want to use two different functions for this and wanted a little bit more information, such as what type of information is stored in each of these variables (columns)? To get that information, we’d turn to the function str(), which provides us information about the structure of the dataset. ## display structure str(df) str() output The output here still tells us the size of df and the variable names, but we also learn what the class of each variable is and see a few of the values for each of the variables. A very similar function to str() is the glimpse() function from the dplyr package. As you’ve been introduced to this function previously, we just wanted to remind you that glimpse() can also be used to understand the size and structure of your data frame ## install and load package install.packages(&quot;dplyr&quot;) library(dplyr) ## get a glimpse of your data glimpse(df) glimpse() output 4.0.1.2 Missing Values In any analysis after your descriptive analysis, missing data can cause a problem. Thus, it’s best to get an understanding of missingness in your data right from the start. Missingness refers to observations that are not included for a variable. In R, NA is the preferred way to specify missing data, so if you’re ever generating data, its best to include NA wherever you have a missing value. However, individuals who are less familiar with R code missingness in a number of different ways in their data: -999, N/A, ., or a blank space. As such, it’s best to check to see how missingness is coded in your dataset. A reminder: sometimes different variables within a single dataset will code missingness differently. This shouldn’t happen, but it does, so always use caution when looking for missingness. In this dataset, all missing values are coded as NA, and from the output of str(df) (or glimpse(df)), we see that at least a few variables have NA values. We’ll want to quantify this missingness though to see which variables have missing data and how many observations within each variable have missing data. To do this, we can write a function that will calculate missingness within each of our variables. To do this we’ll combine a few functions. In the code here is.na() returns a logical (TRUE/FALSE) depending upon whether or not the value is missing (TRUE if it is missing). sum() then calculates the number of TRUE values there are within an observation. We wrap this into a function and then use sapply() to calculate the number of missing values in each variable. The second bit of code does the exact same thing but divides those numbers by the total number of observations (using nrow(df). For each variable, this returns the proportion of missingness: ## calculate how many NAs there are in each variable sapply(df, function(x) sum(is.na(x))) ## calculate the proportion of missingness ## for each variable sapply(df, function(x) sum(is.na(x)))/nrow(df) missingness in msleep Running this code for our dataframe, we see that many variables having missing values. Specifically, to interpret this output for the variable brainwt, we see that 27 observations have missing data. This corresponds to 32.5% of the observations in the dataset. It’s also possible to visualize missingness so that we can see visually see how much missing data there is and determine whether or not the same samples have missing data across the dataset. You could write a function to do this yourself using ggplot2; however, Nicholas Tierney has already written one for you. He has written two helpful packages for exploratory and descriptive data analyses: naniar and visdat. For our purposes, we’ll just install and load naniar here; however, links to both have been included in the additional resources section at the end of this lesson. We recommend looking through the examples provided in the documentation to see additional capabilities within these packages. ## install naniar package install.packages(&quot;naniar&quot;) library(naniar) ## visualize missingness vis_miss(df) vis_miss() output Here, we see the variables listed along the top with percentages summarizing how many observations are missing data for that particular variable. Each row in the visualization is a different observation. Missing data are black. Non-missing values are in grey. Focusing again on brainwt, we can see the 27 missing values visually. We can also see that sleep_cycle has the most missingness, while many variables have no missing data. The relative missingness within a dataset can be easily captured with another function from the naniar package: gg_miss_var(): ## visualize relative missingness gg_miss_var(df) + theme_bw() gg_miss_var() output Here, the variables are listed along the left-hand side and the number of missing values for each variable is plotted. We can clearly see that brainwt has 27 missing values in this dataset, while sleep_cycle has the most missingness among variables in this dataset. Getting an understanding of what values are missing in your dataset is critical before moving on to any other type of analysis. 4.0.1.3 Shape Determining the shape of your variable is essential before any further analysis is done. Statistical methods used for inference (discussed in a later lesson) often require your data to be distributed in a certain manner before they can be applied to the data. Thus, being able to describe the shape of your variables is necessary during your descriptive analysis. When talking about the shape of one’s data, we’re discussing how the values (observations) within the variable are distributed. Often, we first determine how spread out the numbers are from one another (do all the observations fall between 1 and 10? 1 and 1000? -1000 and 10?). This is known as the range of the values. The range is described by the minimum and maximum values taken by observations in the variable. After establishing the range, we determine the shape or distribution of the data. More explicitly, the distribution of the data explains how the data are spread out over this range. Are most of the values all in the center of this range? Or, are they spread out evenly across the range? There are a number of distributions used commonly in data analysis to describe the values within a variable. We’ll cover just a few of them in this lesson, but keep in mind this is certainly not an exhaustive list. 4.0.1.3.1 Normal Distribution The Normal distribution (also referred to as the Gaussian distribution) is a very common distribution and is often described as a bell-shaped curve. In this distribution, the values are symmetric around the central value with a high density of the values falling right around the central value. The left hand of the curve mirrors the right hand of the curve. Normal Distribution A variable can be described as normally distributed if: there is a strong tendency for data to take a central value - many of the observations are centered around the middle of the range deviations away from the central value are equally likely in both directions the frequency of these deviations away form the central value occurs at the same rate on either side of the central value. Taking a look at the sleep_total variable within our example dataset, we see that the data are somewhat normal; however, they aren’t entirely symmetric. ggplot(df, aes(sleep_total)) + geom_density() distribution of msleep sleep_total A variable that is distributed more normally can be seen in the iris dataset, when looking at the Sepal.Width variable. ggplot(iris, aes(Sepal.Width)) + geom_density() distribution of iris Sepal.Width 4.0.1.3.2 Skewed Distribution Alternatively, sometimes data follow a skewed distribution. In a skewed distribution, most of the values fall to one end of the range, leaving a tail off to the other side. When the tail is off to the left, the distribution is said to be skewed left. When off to the right, the distribution is said to be skewed right. Skewed Distributions To see an example from the msleep dataset, we’ll look at the variable sleep_rem. Here we see that the data are skewed right, given the shift in values away from the right, leading to a long right tail. Here, most of the values are at the lower end of the range. ggplot(df, aes(sleep_rem)) + geom_density() sleep_rem is skewed right 4.0.1.3.3 Uniform Distribution Finally, in distributions we’ll discuss today, sometimes values for a variable are equally likely to be found along any portion of the distribution. The curve for this distribution looks more like a rectangle, since the likelihood of an observation taking a value is constant across the range of possible values. Uniform Distribution 4.0.1.3.4 Outliers Now that we’ve discussed distributions, it’s important to discuss outliers – or an observation that falls far away from the rest of the observations in the distribution. If you were to look at a density curve, you could visually identify outliers as observations that fall far from the rest of the observations. density curve with an outlier For example, imagine you had a sample where all of the individuals in your sample are between the ages of 18 and 65, but then you have one sample that is 1 year old and another that is 95 years old. Sample population If we were to plot the age data on a density plot, it would look something like this: example densityplot The baby and elderly individual would pop out as outliers on the plot. After identifying outliers, one must determine if the outlier samples should be included or removed from your dataset? This is something to consider when carrying out an analysis. caution It can sometimes be difficult to decide whether or not a sample should be removed from the dataset. In the simplest terms, no observation should be removed from your dataset unless there is a valid reason to do so. For a more extreme example, what if that dataset we just discussed (with all the samples having ages between 18 and 65) had one sample with the age 600? Well, if these are human data, we clearly know that is a data entry error. Maybe it was supposed to be 60 years old, but we may not know for sure. If we can follow up with that individual and double-check, it’s best to do that, correct the error, make a note of it, and continue you with the analysis. However, that’s often not possible. In the cases of obvious data entry errors, it’s likely that you’ll have to remove that observation from the dataset. It’s valid to do so in this case since you know that an error occurred and that the observation was not accurate. Outliers do not only occur due to data entry errors. Maybe you were taking weights of your observations over the course of a few weeks. On one of these days, your scale was improperly calibrated, leading to incorrect measurements. In such a case, you would have to remove these incorrect observations before analysis. Outliers can occur for a variety of reasons. Outliers can occur due human error during data entry, technical issues with tools used for measurement, as a result of weather changes that affect measurement accuracy, or due to poor sampling procedures. It’s always important to look at the distribution of your observations for a variable to see if anything is falling far away from the rest of the observations. If there are, it’s then important to think about why this occurred and determine whether or not you have a valid reason to remove the observations from the data. An important note is that observations should never be removed just to make your results look better. Wanting better results is not a valid reason for removing observations from your dataset. 4.0.1.3.4.1 Identifying Outliers To identify outliers visually, density plots and boxplots can be very helpful. For example, if we returned to the iris dataset and looked at the distribution of Petal.Length, we would see a bimodal distribution (yet another distribution!). Bimodal distributions can be identified by density plots that have two distinct humps. In these distributions, there are two different modes – this is where the term “bimodal” comes from. In this plot, the curve suggests there are a number of flowers with petal length less than 2 and many with petal length around 5. ## density plot library(ggplot) ggplot(iris, aes(Petal.Length))+ geom_density() iris density plot Since the two humps in the plot are about the same height, this shows that it’s not just one or two flowers with much smaller petal lengths, but rather that there are many. Thus, these observations aren’t likely an outlier. To investigate this further, we’ll look at petal length broken down by flower species: ## box plot ggplot(iris, aes(Species, Petal.Length))+ geom_boxplot() iris boxplot In this boxplot, we see in fact that setosa have a shorter petal length while virginica have the longest. Had we simply removed all the shorter petal length flowers from our dataset, we would have lost information about an entire species! Boxplots are also helpful because they plot “outlier” samples as points outside the box. By default, boxplots define “outliers” as observations as those that are 1.5 x IQR (interquartile range). The IQR is the distance between the first and third quartiles. This is a mathematical way to determine if a sample may be an outlier. It is visually helpful, but then it’s up to the analyst to determine if an observation should be removed. While the boxplot identifies outliers in the setosa and versicolor species, these values are all within a reasonable distance of the rest of the values, and unless I could determine why this occurred, I would not remove these observations from the dataset iris boxplot with annotations 4.0.1.4 Central Tendency Once you know how large your dataset is, what variables you have information on, how much missing data you’ve got for each variable, and the shape of your data, you’re ready to start understanding the information within the values of each variable. Some of the simplest and most informative measures you can calculate on a numeric variable are those of central tendency. The two most commonly used measures of central tendency are: mean and median. These measures provide information about the typical or central value in the variable. 4.0.1.4.1 mean The mean (often referred to as the average) is equal to the sum of all the observations in the variable divided by the total number of observations in the variable. The mean takes all the values in your variable and calculates the most common value. So if you had the following vector: a &lt;- c(1, 2, 3, 4, 5, 6), the mean would be 3.5. calculating the mean But what if we added another ‘3’ into that vector, so that it were: a &lt;- c(1, 2, 3, 3, 4, 5, 6). Now, the mean would be 3.43. It would decrease the average for this set of numbers as you can see in the calculations here: decreased average To calculate the mean in R, the function is mean(). Here, we show how to calculate the mean for a variable in R. Note that when you have NAs in a variable, you’ll need to let R know to remove the NAs (using na.rm=TRUE) before calculating your mean. Otherwise, it will return NA. ## this will return NA mean(df$sleep_cycle) ## have to tell R to ignore the NAs mean(df$sleep_cycle, na.rm=TRUE) mean(sleep_cycle) 4.0.1.4.2 median The median is the middle observation for a variable after the observations in that variable have been arranged in order of magnitude (from smallest to largest). The median is the middle value. Using the same vector as we first use to calculate median, we see that the middle value for this set of numbers is 3.5 as this is the value at the center of this set of numbers. This happens to be the same value as the mean was. However, that is not always the case. When we add that second 3 in the middle of the set of numbers, the median is now 3, as this is the value at the center of this set of numbers. 3 is the middle value. medians To calculate the median in R, use the function median(). Again, when there are NAs in the variable, you have to tell R explicitly to remove them before calculating the median. ## calculate the median median(df$sleep_cycle, na.rm=TRUE) median sleep_cycle While not the exact same value, the mean and median for sleep_cycle are similar (0.44 and 0.33). However, this is not always the case. For data that are skewed or contain outlier values – values that are very different from the rest of the values in the variable – the mean and the median will be very different from one another. In our example dataset, the mean and the median values for the variable bodywt are quite different from one another. ## calculate mean and median mean(df$bodywt) median(df$bodywt) ## look at the histogram ggplot(df, aes(bodywt)) + geom_histogram() mean vs median When we look at the histogram of the data, we see that most body weights are less than 200 lbs. Thus, the median, or value that would be in the middle if you lined all the weights up in order, is 1.6 kilograms. However, there are a few mammals that are a lot bigger than the rest of the animals. These mammals are outliers in the dataset. These outliers increase the mean. These larger animals drive the mean of the dataset to 166 kilograms. When you have outliers in the dataset, the median is typically the measure of central tendency you’ll want to use, as it’s resistant to the effects of outlier values. 4.0.1.4.3 mode There is a third, less-frequently calculated measure of central tendency for continuous variables, known as the mode. This is the value that comes up most frequently in your dataset. For example, if your dataset a were comprised of the following numbers a &lt;- c(0, 10, 10, 3, 5, 10, 10), 10 would be the mode, as it occurs four times. It doesn’t matter whether it’s the largest value, the smallest value, or somewhere in between, the most frequently value in your dataset is the mode. There is no built-in function for calculating the mode in R for a numeric value, which should suggest that, for continuous variables, knowing the mode of a variable is often less crucial than knowing the mean and median (which is true)! However, you could write a function to calculate it. For the above vector a, which.max(tabulate(a)) would return the mode: 10. (Note that this would not work if you had two values that were found in the dataset at the same frequency. A more eloquent approach would be required.) mode of a continuous variable However, for categorical variables, the level with the most observations would be the mode. This can be determined using the table() function, which breaks down the number of observations within the categorical variable table() output Further, the mode for a categorical variable can be visualized by generating a barplot: ## plot categorical variable to visualize mode ggplot(df, aes(order)) + geom_bar() + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) geom_bar visually displays the mode 4.0.1.5 Variability In addition to measures of central tendency, measures of variability are key in describing the values within a variable. Two common and helpful measures of variability are: standard deviation and variance. Both of these are measures of how spread out the values in a variable are. 4.0.1.5.1 Variance The variance tells you how spread out the values are. If all the values within your variable are exactly the same, that variable’s variance will be zero. The larger your variance, the more spread out your values are. Take the following vector and calculate its variance in R using the var() function: ## variance of a vector where all values are the same a &lt;- c(29, 29, 29, 29) var(a) ## variance of a vector with one very different value b &lt;- c(29, 29, 29, 29, 723678) var(b) variance The only difference between the two vectors is that the second one has one value that is much larger than “29”. The variance for this vector is thus much higher. 4.0.1.5.2 Standard Deviation By definition, the standard deviation is the square root of the variance, thus if we were to calculate the standard deviation in R using the sd() function, we’d see that the sd() function is equal to the square root of the variance: ## calculate standard deviation sd(b) ## this is the same as the square root of the variance sqrt(var(b)) Standard Deviation For both measures of variance, the minimum value is 0. The larger the number, the more spread out the values in the valuable are. 4.0.2 Summarizing Your Data Often, you’ll want to include tables in your reports summarizing your dataset. These will include the number of observations in your dataset and maybe the mean/median and standard deviation of a few variables. These could be organized into a table using what you learned in the data visualization course about generating tables. 4.0.2.1 skimr Alternatively, there is a helpful package that will summarize all the variables within your dataset. The skimr package provides a tidy output with information about your dataset. To use skimr, you’ll have to install and load the package before using the helpful function skim() to get a snapshot of your dataset. install.packages(&quot;skimr&quot;) library(skimr) skim(df) skim() output The output from skim separately summarizes categorical and continuous variables. For continuous variables you get information about the mean and median (p50) column. You know what the range of the variable is (p0 is the minimum value, p100 is the maximum value for continuous variables). You also get a measure of variability with the standard deviation (sd). It even quantifies the number of missing values (missing) and shows you the distribution of each variable (hist)! This function can be incredibly useful to get a quick snapshot of what’s going on with your dataset. 4.0.2.2 Published Descriptive Analyses In academic papers, descriptive analyses often lead to the information included in the first table of the paper. These tables summarize information about the samples used for the analysis in the paper. Here, we’re looking at the first table in a paper published in the New England Journal of Medicine by The CATT Research Group. Table 1 We can see that there is a lot of descriptive information being summarized in this table just by glancing at it. If we zoom in and just focus on the top of the table, we see that the authors have broken down a number of the variables (the rows) and summarized how many patients they had in each of their experimental categories (the columns). Focusing on Sex specifically, we can see that there were 183 females and 118 males in their first experimental group. In the parentheses, they summarize what percent of their sample that was. In this same category, the sample was 60.8% female and 39.2% male. Table 1 - just the top We provide this here as an example of how someone would include a descriptive analysis in a report or publication. It doesn’t always have to be this long, but you should always describe your data when sharing it with others. 4.0.3 Summary This lesson covered the necessary parts of carrying out a descriptive analysis. Generally, describing a dataset involves describing the numbers of observations and variables in your dataset, getting an understanding of missingness, and, understanding the shape, central tendency, and variability of each variable. Description must happen as the first step in any data analysis. 4.0.4 Additional Resources Visualizing Incomplete &amp; Missing Data, by Nathan Yau Getting Started with the naniar package, from Nicholas Tierney visdat package, also from Nicholas Tierney to further visualize datasets during exploratory and descriptive analyses Using the skimr package, by Elin Waring 4.0.5 Slides and Video Automated Videos Slides "],["exploratory-analysis.html", "Chapter 5 Exploratory Analysis", " Chapter 5 Exploratory Analysis The goal of an exploratory analysis is to examine, or explore the data and find relationships that weren’t previously known. Exploratory analyses explore how different measures might be related to each other but do not confirm that relationship as causal, i.e., one variable causing another. You’ve probably heard the phrase “Correlation does not imply causation,” and exploratory analyses lie at the root of this saying. Just because you observe a relationship between two variables during exploratory analysis, it does not mean that one necessarily causes the other. Because of this, exploratory analyses, while useful for discovering new connections, should not be the final say in answering a question! It can allow you to formulate hypotheses and drive the design of future studies and data collection, but exploratory analysis alone should never be used as the final say on why or how data might be related to each other. In short, exploratory analysis helps us ask better questions, but it does not answer questions. More specifically, we explore data in order to: Understand data properties such as nonlinear relationships, the existence of missing values, the existence of outliers, etc. Find patterns in data such as associations, group differences, confounders, etc. Suggest modeling strategies such as linear vs. nonlinear models, transformation “Debug” analyses Communicate results 5.0.1 General principles of exploratory analysis We can summarize the general principles of exploratory analysis as follows: Look for missing values Look for outlier values Use plots to explore relationships Use tables to explore relationships If necessary, transform variables These principles may be more clear in an example. We will use a dataset from Kaggle.com that contains 120 years of Olympics history on athletes and results. If you don’t have an account on Kaggle, create one and go to the link https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results and under “Data Sources” download the athlete_events.csv to your computer. Dataset on 120 years of Olympics history on athletes and results Upload the data on RStudio.cloud and import the CSV file using the commands you have learned. Unfortunately, you cannot download the CSV file directly from the web address since downloading datasets on Kaggle requires logging in. Importing data using read_csv() As we learned before, we can use the package skimr to take a look at the data. Using the skimr package to have a summary of the data We see that the dataset contains 15 variables and 271,116 observations. Some of the variables are of factor type and others are of integer or numeric type. The dataset includes variables on athletes such as name, sex, the sport played, whether they received a medal, age, and height. We first need to understand the data properties. So let’s start with missing values. We have different types of variables in our data First, the results of the skim() function indicate that some of our variables have lots of missing values. For instance, the variable Medal has 231,333 missing values. Generally, this is a place for concern since most statistical analyses ignore observations with missing values. However, it is obvious that the missing values for the variable Medal are mainly because the athlete didn’t receive any medals. So this kind of missing value should not be a problem. However, we have missing values in the variables Height and Age. Since we are going to use these variables in our analysis in this lesson, observations with missing values for these two variables will be dropped from our analysis. Remember that NA is the most common character for missing values, but sometimes they are coded as spaces, 999, -1 or “missing”. Check for missing values in a variety of ways. There are some missing values in the data Second, we can see that there are some outliers in some of the numerical variables. For example, look at the summary of the variable Age. Although the average age among all the athletes is around 25, there is an individual who is 97 years old (fun fact: use the command subset(df, df$Age == 97) to check out the information about this athlete. You will see that the name of the athlete is John Quincy Adams Ward and he competed in the sport(!) Art Competitions Mixed Sculpturing in 1928. This artist is known for his George Washington statue in front of Federal Hall in Wall Street in New York City.) It is always good to know about the existence of outliers in your sample. Outliers can significantly skew the results of your analysis. You can find outliers by looking at the distribution of your variable too. There is an outlier in the Age variable Histograms, in general, are one of the best ways to look at a variable and find abnormalities. You can see that the age of most individuals in the sample are between 18-35. Histogram of the variable Age Now, rather than just summarizing the data points within a single variable, we can look at how two or more variables might be related to each other. For instance, we like to know if there is an association between age of athletes and their gender. One of the ways to do this is to look at a boxplot of age grouped by gender, i.e., the distribution of age separated for male and female athletes. Boxplot shows the distribution of the variable age for the gender groups. You can see that the average age is slightly higher for men than for women. Boxplot of the variable Age for male and female individuals If we are interested in looking at the distribution of male and female athletes over time, we can use frequency tables. Let us first create a frequency table of the share of women in each Olympic event. Tables are good for looking at factor or character variables. Wrangling data to find the share of female athletes over time Now, if we want to plot this trend, we can use geom_line() from ggplot. It’s interesting that the share of women among all athletes that was once at a very low level in the early 1900s has gone up to almost 50% in modern times. Plot of the share of female athletes over time In general, the most important plots in exploratory data analysis are: Scatterplots (geom_point()) Histograms (geom_histogram()) Density plots (geom_density()) Boxplots (geom_boxplot()) Barplots (geom_bar()) To end our lesson on exploratory analysis, let’s consider a question: are taller athletes more likely to win a medal? To answer this question we can use different methods. We can look at the distribution of height for those who received a medal and those who didn’t. We can use boxplots or barplots. The choice is yours but because boxplots are more informative, we will use them. We can first create a variable that indicates whether the athlete has any medal (the variable Medal indicates the type of medals). Note that the variable has.medal is a transformation of the variable Medal. Creating a variable that shows whether the athlete has a medal or not And now, we use the following code to create the boxplot. Boxplot for the relationship between height and having won a medal What is obvious is that those who have a medal are taller. Can we say that being tall increases the probability of winning a medal in the Olympics? The answer to this question is that we don’t know. There are some possible scenarios. For instance, it could be true that being tall increase the chances of winning medals. But it could also be that there are more medals awarded in sports such as volleyball or basketball that require taller athletes. In these sports, every member of the winning team gets a medal (even if country counts only one medal is counted for the country). As a result, we may end up having so many tall athletes with a medal in each Olympics. It could also be that there are other confounding factors involved that explain why an athlete wins a medal. We will learn about confounding variables in future lessons. For now, it’s important to know, as we said in the beginning of this lesson, that association or correlation does not mean causation. 5.0.2 Slides and Video Automated Videos Slides "],["inference-overview.html", "Chapter 6 Inference: Overview", " Chapter 6 Inference: Overview Inferential Analysis is what analysts carry out after they’ve described and explored their data set. After understanding your dataset better, analysts often try to infer something from the data. This is done using statistical tests. While we’ll only be discussing linear regression in this lesson, there are many different statistical tests, each of which (when appropriately applied) can be used for inferential data analysis. We’ll cover inference in general in this lesson and then cover more of the details you’ll need to truly understand regression in the following few lessons. Across all the lessons discussing inference, we’ll cover: Random Sampling &amp; Inference Uncertainty Simple Linear Regression Multiple Linear Regression Confounding Correlation While it is certainly a lot, it will be incredibly important for you to understand this material going forward, so take your time on this overview and be sure to understand each paragraph, example, and image. Then, in the following lessons, you’ll be able to learn more inference concepts, work through additional examples, and get more practice with inferential analysis. 6.0.1 Getting Started with Inference Ok, so let’s break this all down a little bit. The goal of inferential analyses is to use a relatively small sample of data to infer or say something about the population at large. This is required because often we want to answer questions about a population. Let’s take a dummy example here where we have a population of 14 shapes. Here, in this graphic, the shapes represent individuals in the population and the colors of the shapes can be either pink or grey: The population In this example we only have fourteen shapes in the population; however, in inferential data analysis, it’s not usually possible to sample everyone in the population. Consider if this population were everyone in the United States or every college student in the world. As getting information from every individual would be infeasible. Data are instead collected on a subset, or a sample of the individuals in the larger population. A sample is collected from the population In our example, we’ve been showing you how many pink and how many gray shapes are in the larger population. However, in real life, we don’t know what the answer is in the larger population. That’s why we collected the sample! We don’t know what the truth is in the population This is where inference comes into play. We analyze the data collected in our sample and then do our best to infer what the answer is in the larger population. In other words, inferential data analysis uses data from a sample to make its best guess as to what the answer would be in the population if we were able to measure every individual. Inference from the sample makes its best guess as to what the truth is in the population 6.0.2 Uncertainty Because we haven’t directly measured the population but have only been able to take measurements on a sample of the data, when making our inference we can’t be exactly sure that our inference about the population is exact. For example, in our sample one-third of the shapes are grey. We’d expect about one-third of the shapes in our population to be grey then too! Well, one-third of 14 (the number of shapes in our population) is 4.667. Does this mean four shapes are truly gray? Inference gives us a good guess as to what the truth is in the population Or maybe five shapes in the population are grey? Maybe the population really has five grey shapes in it… Given the sample we’ve taken, we can guess that 4-5 shapes in our population will be grey, but we aren’t certain exactly what that number is. In statistics, this “best guess” is known as an estimate. This means that we estimate that 4.667 shapes will be gray. But, there is uncertainty in that number. Because we’re taking our best guess at figuring out what that estimate should be, there’s also a measure of uncertainty in that estimate. Inferential data analysis includes generating the estimate and the measure of uncertainty around that estimate. Let’s return back to the example where we know the truth in the population. Hey look! There were actually only three grey shapes after all. It is totally possible that if you put all those shapes into a bag and pulled three out that two would be pink and one would be grey. As statisticians, we’d say that getting this sample was probable (it’s within the realm of possibility), but it’s not the most likely (The most likely was either 4 or 5.) This really drives home why it’s important to add uncertainty to your estimate whenever you’re doing inferential analysis! There actually were only three grey shapes in our population after all 6.0.3 Random Sampling Since you are moving from a small amount of data and trying to generalize to a larger population, your ability to accurately infer information about the larger population depends heavily on how the data were sampled. We briefly touched on this in a previous lesson, but let’s discuss it fully now. The data in your sample must be representative of your larger population to be used for inferential data analysis. Let’s discuss what this means. Using the same example, what if, in your larger population, you didn’t just have grey and pink shapes, but you also had blue shapes? What if your larger population had three different color shapes? Well, if your sample only has pink and grey shapes, when you go to make an inference, there’s no way you’d infer that there should be blue shapes in your population since you didn’t capture any in your sample. In this case, your sample is not representative of your larger population. In cases where you do not have a representative sample, you can not carry out inference, since you will not be able to correctly infer information about the larger population. You can only carry out an inferential analysis when your sample is representative of the population This means that you have to design your analysis so that you’re collecting representative data and that you have to check your data after data collection to make sure that you were successful. You may at this point be thinking to yourself. “Wait a second. I thought I didn’t know what the truth was in the population. How can I make sure it’s representative?” Good point! With regards to the measurement you’re making (color distribution of the shapes, in this example), you don’t know the truth. But, you should know other information about the population. What is the age distribution of your population? Your sample should have a similar age distribution. What proportion of your population is female? If it’s half, then your sample should be comprised of half females. Your data collection procedure should be set up to ensure that the sample you collect is representative (very similar to) your larger population. Then, once the data are collected, your descriptive analysis should check to ensure that the data you’ve collected are in fact representative of your larger population. By randomly sampling your larger population, then ensures that the inference you make about the measurement of interest (color distribution of the shapes) will be most accurate. To reiterate: If the data you collect is not from a representative sample of the population, the generalizations you infer won’t be accurate for the population. 6.0.4 A real-life example of inferential data analysis Unlike in our previous examples, Census data wouldn’t be used for inferential analysis. By definition, a census already collects information on (functionally) the entire population. Thus, there is no population on which to infer. Census data are the rare exception where a whole population is included in the dataset. Further, using data from the US census to infer information about another country would not be a good idea because the US isn’t necessarily representative of the other country. Instead, a better example of a dataset on which to carry out inferential analysis would be the data used in the study: The Effect of Air Pollution Control on Life Expectancy in the the United States: An Analysis of 545 US counties for the period 2000 to 2007. In this study, researchers set out to understand the effect of air pollution on everyone in the United States Study question looks to learn something about entire US population To answer this question, a subset of the US population was studied, and the researchers looked at the level of air pollution experienced and life expectancy. It would have been nearly impossible to study every individual in the United States year after year. Instead, this study used the data they collected from a sample of the US population to infer how air pollution might be impacting life expectancy in the entire US! Studies use representative samples to infer information about the larger population 6.0.5 Summary In this first inference lesson, we introduced simply an overview of inferential thinking. We introduced the idea that if you want to learn something about a population, often analyzing a representative sample of the population can help get you to your answer. With this conceptual overview, in the following lessons we’ll see what that looks like at the level of code and how to interpret the output. 6.0.6 Additional Resources 6.0.7 Slides and Video Automated Video Slides "],["inference-linear-regression.html", "Chapter 7 Inference: Linear Regression", " Chapter 7 Inference: Linear Regression Inferential analysis is commonly the goal of statistical modeling, where you have a small amount of information to extrapolate and generalize that information to a larger group. One of the most common approaches used in statistical modeling is known as linear regression. Here, we’ll discuss when using linear regression is appropriate, how to carry out the analysis in R, and how to interpret the results from this statistical approach. When discussing linear regression, we’re trying to describe (model) the relationship between a dependent variable and an independent variable. linear regression models relationship between two variables When visualizing a linear relationship, the independent variable is plotted along the bottom of the graph, on the x-axis and the dependent variable is plotted along the side of the plot, on the y-axis. independent on the x-axis; dependent on the y-axis When carrying out linear regression, a best-fitting line is drawn through the data points to describe the relationship between the variables. A best-fitting line describes the relationship between the variables A best-fitting line, technically-speaking, minimizes the sum of the squared errors. In simpler terms, this means that the line that minimizes the distance of all the points from the line is the best-fitting line. Or, most simply, there are the same number of points above the line as there are below the line. In total, the distance from the line for the points above the line will be the same as the distance from the points to the line below the line. Note that the best fitting line does not have to go through any points to be the best-fitting line. Here, on the right, we see a line that goes through seven points on the plot (rather than the four the best-fitting line goes through, on the left). However, this is not a best-fitting line, as there are way more points above the line than there are below the line. A best-fitting line does NOT have to go through the most points possible This line describes the relationship between the two variables. If you look at the direction of the line, it will tell you whether there is a positive or a negative relationship between the variables. In this case, the larger the value of the independent variable, the larger the value of the dependent variable. Similarly, the smaller the value of the independent variable, the smaller the value of the dependent variable. When this is the case, there is a positive relationship between the two variables. A positive relationship will have points that trend up and to the right An example of variables that have a positive relationship would be the height of fathers and their sons. In general, the taller a father is, the taller his son will be. And, the shorter a father is the more likely his son is to be short. Father and son height demonstrate a positive linear relationship Alternatively, when the higher the value of the independent variable, the lower the value of the dependent variable, this is a negative relationship. A positive relationship will have points that trend up and to the left An example of variables that have a negative relationship would be the relationship between a students’ absences and their grades. The more absences a student has, the lower their grades tend to be. Student absences and grades show a negative linear relationship Linear regression, in addition to to describing the direction of the relationship, it can also be used to determine the strength of that relationship. This is because the assumption with linear regression is that the true relationship is being described by the best-fitting line. Any points that fall away from the line do so due to random error. This means that if all the points fall directly on top of the line, there is no error. The further the points fall from the line, the greater the error. When points are further from the best-fitting line, the relationship between the two variables is weaker than when the points fall closer to the line. Correlation is weaker on the left and stronger on the right In this example, the pink line is exactly the same best-fitting line in each graph. However, on the left, where the points fall further from the line, the strength of the relationship between these two variables is weaker than on the right, where the points fall closer to the line, where the relationship is stronger. The strength of this relationship is measured using correlation. The closer the points are to the line the more correlated the two variables are, meaning the relationship between the two variables is stronger. 7.0.0.1 Assumptions Thus far we have focused on drawing linear regression lines. Linear regression lines can be drawn on any plot, but just because you can do something doesn’t mean you actually should. When it comes to linear regression, in order to carry our any inference on the relationship between two variables, there are a few assumptions that must hold before inference from linear regression can be done. The two assumptions of simple linear regression are linearity and homoscedasticity. 7.0.0.1.1 Linearity The relationship between the two variables must be linear. For example, what if we were plotting data from a single day and we were looking at the relationship between temperature and time. Well, we know that generally temperature increases throughout the day and then decreases in the evening. Here, we see some example data reflective of this relationship. The upside-down u-shape of the data suggests that the relationship is not in fact linear. While we could draw a straight line through these data, it would be inappropriate. In cases where the relationship between the variables cannot be well-modeled with a straight line, linear regression should not be used. If the relationship between the variables is non-linear, regression should not be used 7.0.0.1.2 Homoscedasticity In addition to displaying a linear relationship, the random variables must demonstrate homoscedasticity. In other words, the variance (distance from the line) must be constant throughout the variable. If points at one end are much closer to the best-fitting line than points are at the other end, homoscedasticity has been violated and linear regression is not appropriate for the data. Variance must be consistent across the variable for linear regression to be used 7.0.0.1.3 Normality of residuals When we fit a linear regression, typically the data do not fall perfectly along the regression line. Rather, there is some distance from each point to the line. Some points are quite close to the line, while others are further away. Each point’s distance to the regression line can be calculated. This distance is the residual measurement. Residuals are calculated by measuring the distance from each point to the regression line In linear regression, one assumption is that these residuals follow a Normal distribution. This means that if you were to calculate each residual (each point’s distance to the regression line) and then plot a histogram of all of those values - that plot should look like a Normal Distribution. Linear regression assumes normality of residuals If you do not see normality of residuals, this can suggest that outlier values - observations more extreme than the rest of the data - may exist in your data. This can severely affect you regression results and lead you to conclude something that is untrue about your data. Thus, it is your job, when running linear regression to check for: Non-linearity Heteroscedasticity Outlier values Normality of residuals We’ll discuss how to use diagnostic plots below to check that these assumptions have been met and that outlier values are not severely affecting your results. 7.0.1 What can linear regression infer? Now that we understand what linear regression is and what assumptions must hold for its use, when would you actually use it? Linear regression can be used to answer many different questions about your data. Here we’ll discuss specifically how to make inferences about the relationship between two numeric variables. 7.0.1.1 What is the association? Often when people are carrying out linear regression, they are looking to better understand the relationship between two variables. When looking at this relationship, analysts are specifically asking “What is the association between these two variables?” Association between variables describes the trend in the relationship (positive, neutral, or negative) and the strength of that relationship (how correlated the two variables are). After determining that the assumptions of linear regression are met, in order to determine the association between two variables, one would carry out a linear regression. From the linear regression, one would then interpret the Beta estimate and the standard error from the model. Beta estimate - determines the direction and strength of the relationship between the two variables. A beta of zero suggests there is no association between the two variables. However, if the beta value is positive, the relationship is positive. If the value is negative, the relationship is negative. Further, the larger the number, the bigger the effect is. We’ll discuss effect size and how to interpret the value in more detail later in this lesson. Beta estimates describe the size and strength of the effect Standard error - determines how uncertain the beta estimate is. The larger the standard error, the more uncertain we are in the estimate. The smaller the standard error, the less uncertain we are in the estimate. Standard errors are calculated based on how well the best-fitting line models the data. The closer the points are to the line, the lower the standard error will be, reflecting our decreased uncertainty. However, as the points are further from the regression line, our uncertainty in the estimate will increase, and the standard error will be larger. Standard errors explain how uncertain the estimate is A reminder that when carrying out inferential data analysis, you will always want to report an estimate and a measure of uncertainty. For linear regression, this will be the beta estimate and the standard error. You may have heard talk of p-values at some point. People tend to use p-values to describe the strength of their association due to its simplicity. The p-value is a single number that takes into account both the estimate (beta estimate) and the uncertainty in that estimate (SE). The lower a p-value the more significant the association is between two variables. However, while it is a simple value, it doesn’t tell you nearly as much information as reporting the estimates and standard errors directly. Thus, if you’re reporting p-values, it’s best to also include the estimate and standard errors as well. That said, the general interpretation of a p-value is “the probability of getting the observed results (or results more extreme) by chance alone.” Since it’s a probability, the value will always be between 0 and 1. Then, for example, a p-value of 0.05, means that 5 percent of the time (or 1 in 20), you’d observe results this extreme simply by chance. 7.0.1.1.1 Association Testing in R Now that we’ve discussed what you can learn from an association test, let’s look at an example in R. For this example we’ll use the trees dataset available in R, which includes girth, height, and volume measurements for 31 black cherry trees. With this dataset, we’ll answer the question: Can we infer the height of a tree given its girth? Presumably, it’s easier to measure a trees girth (width around) than it is to measure its height. Thus, here we want to know whether or not height and girth are associated. In this case, since we’re asking if we can infer height from girth, girth is the independent variable and height is the dependent variable. In other words, we’re asking does height depend on girth? First, before carrying out the linear regression to test for association and answer this question, we have to be sure linear regression is appropriate. We’ll test for linearity and homoscedasticity. To do so, we’ll first use ggplot2 to generate a scatterplot of the variables of interest. library(ggplot2) ggplot(trees) + geom_point(aes(Height, Girth)) scatterplot of trees dataset From the looks of this plot, the relationship looks approximately linear, but to visually make this a little easier, we’ll add a line of best first to the plot. ggplot(trees, aes(Height, Girth)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) scatterplot with line of best fit On this graph, the relationship looks approximately linear and the variance (distance from points to the line) is constant across the data. Given this, it’s appropriate to use linear regression for these data. Fitting the model Now that that’s established, we can run the linear regression. To do so, we’ll use the lm() function to fit the model. The syntax for this function is lm(dependent_variable ~ independent_variable, data = dataset). ## run the regression fit &lt;- lm(Girth ~ Height , data = trees) Model Diagnostics Above, we discussed a number of assumptions of linear regression. After fitting a model, it’s necessary to check the model to see if the model satisfies the assumptions of linear regression. If the model does not fit the data well (for example, the relationship is nonlinear), then you cannot use and interpret the model. In order to assess your model, a number of diagnostic plots can be very helpful. Diagnostic plots can be generated using the plot() function with the fitted model as an argument. par(mfrow = c(2, 2)) plot(fit) This generates four plots: diagnostic linear regression plots using plot() Residuals vs Fitted - checks linear relationship assumption of linear regression. A linear relationship will demonstrate a horizontal red line here. Deviations from a horizontal line suggest nonlinearity and that a different approach may be necessary. Normal Q-Q - checks whether or not the residuals (the difference between the observed and predicted values) from the model are normally distributed. The best fit models points fall along the dashed line on the plot. Deviation from this line suggests that a different analytical approach may be required. Scale-Location - checks the homoscedasticity of the model. A horizontal red line with points equally spread out indicates a well-fit model. A non-horizontal line or points that cluster together suggests that your data are not homoscedastic. Residuals vs Leverage - helps to identify outlier or extreme values that may disproportionately affect the model’s results. Their inclusion or exclusion from the analysis may affect the results of the analysis. Note that the top three most extreme values are identified with numbers next to the points in all four plots. Tree girth and height example In our example looking at the relationship between tree girth and height, we can first check linearity of the data by looking at the Residuals vs Fitted plot. Here, we do see a red line that is approximately horizontal, which is what we’re looking for. Additionally, we’re looking to be sure there is no clear pattern in the points on the plot - we want them to be random on this plot. Clustering of a bunch of points together or trends in this plot would indicate that the data do not have a linear relationship. Residuals plot has data scattered randomly throughout and a mostly horizontal fitted line To check for homogeneity of the variance, we can turn to the Scale-Location plot. Here, we’re again looking for a horizontal red line. In this dataset, there’s a suggestion that there is some heteroscedasticity, with points not being equally far from the regression line across the observations. While not discussed explicitly here in this lesson, we will note that when the data are nonlinear or the variances are not homogeneous (are not homoscedastic), transformations of the data can often be applied and then linear regression can be used. QQ Plots are very helpful in assessing the normality of residuals. Normally distributed residuals will fall along the grey dotted line. Deviation from the line suggests the residuals are not normally distributed.Here, in this example, we do not see the points fall perfectly along the dotted line, suggesting that our residuals are not normally distributed. Deviation from the dotted line suggests non-normality of residuals A histogram (or densityplot) of the residuals can also be used for this portion of regression diagnostics. Here, we’re looking for a Normal distribution of the residuals. library(ggplot2) ggplot(fit, aes(fit$residuals)) + geom_histogram(bins = 5) The QQ Plot and the histogram of the residuals will always give the same answer. Here, we see that with our limited sample size, we do not have perfectly Normally distributed residuals; however, the points do not fall wildly far from the dotted line. Residuals are not perfectly Normally distributed Finally, whether or not outliers (extreme observations) are driving our results can be assessed by looking at the Residuals vs Leverage plot. Generally speaking, standardized residuals greater than 3 or less than -3 are to be considered as outliers. Here, we do not see any values in that range (by looking at the y-axis), suggesting that there are no extreme outliers driving the results of our analysis. Outliers in a dataset can affect the results of your analysis Interpreting the model While the relationship in our example appears to be linear, does not indicate being driven by outliers, is approximately homoscedastic and has residuals that are not perfectly Normally distributed, but fall close to the line in the QQ plot, we can discuss how to interpret the results of the model. ## take a look at the output summary(fit) The summary() function summarizes the model as well as the output of the model. We can see the values we’re interested in in this summary, including the beta estimate, the standard error (SE), and the p-value. Specifically, from the beta estimate, which is positive, we confirm that the relationship is positive (which we could also tell from the scatterplot). We can also interpret this beta estimate explicitly. Beta, SE, and p-value all included in summary() output Specifically, the beta estimate (also known as the beta coefficient or coefficient in the Estimate column) is the amount the dependent variable will change given a one unit increase in he independent variable. In the case of the trees, a beta estimate of 0.256, says that for every inch a tree’s girth increases, its height will increase by 0.256 inches. Thus, we not only know that there’s a positive relationship between the two variables, but we know by precisely how much one variable will change given a single unit increase in the other variable. Note that we’re looking at the second row in the output here, where the row label is “Height”. This row quantifies the relationship between our two variables. The first row quantifies the intercept, or where the line crosses the y-axis. The standard error and p-value are also included in this output. Error is typically something we want to minimize (in life and statistical analyses), so the smaller the error, the more confident we are in the association between these two variables. The beta estimate and the standard error are then both considered in the calculation of the p-value (found in the column Pr[&gt;|t|]). The smaller this value is, the more confident we are that this relationship is not due to random chance alone. Variance Explained Additionally, the strength of this relationship is summarized using the adjusted R-squared metric. This metric explains how much of the variance this regression line explains. The more variance explained, the closer this value is to 1. And, the closer this value is to 1, the closer the points in your dataset fall to the line of best fit. The further they are from the line, the closer this value will be to zero. Adjusted R-squared specifies how closely the data fall are to the regression line As we saw in the scatterplot, the data are not right up against the regression line, so a value of 0.2445 seems reasonable, suggesting that this model (this regression line) explains 24.45% of the variance in the data. Using `broom Finally, while the summary() output are visually helpful, if you want to get any of the numbers out from that model, it’s not always straightforward. Thankfully, there is a package to help you with that! The tidy() function from the broom package helps take the summary output from a statistical model and organize it into a tabular output. #install.packages(&quot;broom&quot;) library(broom) tidy(fit) tidy() helps organize output from statistical models Note that the values haven’t changed. They’re just organized into an easy-to-use table. It’s helpful to keep in mind that this function and package exist as you work with statistical models. Finally, it’s important to always keep in mind that the interpretation of your inferential data analysis is incredibly important. When you use linear regression to test for association, you’re looking at the relationship between the two variables. While girth can be used to infer a tree’s height, this is just a correlation. It does not mean that an increase in girth causes the tree to grow more. Associations are correlations. They are not causal. We’ll discuss this more later in the lesson. For now, however, in response to our question, can we infer a tree’s height from its girth, the answer is yes. We would expect, on average, a tree’s height to increase 0.255 inches for every one inch increase in girth. 7.0.2 Summary In this lesson we’ve discussed not just what inference is generally, but how to carry out an inferential analysis using simple linear regression. We’ve discussed assumptions of linear regression and the interpretation of both the linear regression model itself and the diagnostic plots that accompany this type of analysis. In the following lesson we’ll extend this concept further to discuss confounding, multiple regression, and additional statistical tests. 7.0.3 Slides and Video Automated Video Slides "],["inference-multiple-regression.html", "Chapter 8 Inference: Multiple Regression", " Chapter 8 Inference: Multiple Regression In the last lesson, we finished discussing how to use lm() to assess the association between two numeric variables. However, there is one incredibly important topic that we have to discuss before moving on from linear regression. Confounding in something to watch out for in any analysis you’re doing that looks at the relationship between two more more variables. So…what is confounding? 8.0.1 Confounding Well, let’s consider an example. What if we were interested in understanding the relationship between shoe size and literacy. To do so, we took a look at this small sample of two humans, one who wears small shoes and is not literate and one adult who wears big shoes and is literate. Two humans and their respective shoe sizes and literacy levels If we were to diagram this question, we may ask “Can we infer literacy rates from shoe size?” Possible to infer literacy rate from shoe size? If we return to our sample, it’d be important to note that one of the humans is a young child and the other is an adult. Adult and child with their respective shoe sizes and literacy levels Our initial diagram failed to take into consideration the fact that these humans differed in their age. Age affects their shoe size and their literacy rates. In this example, age is a confounder. Age is a confounder Any time you have a variable that affects both your dependent and independent variables, it’s a confounder. Ignoring confounders is not appropriate when analyzing data. In fact, in this example, you would have concluded that people who wear small shoes have lower literacy rates than those who wear large shoes. That would have been incorrect. In fact, that analysis was confounded by age. Failing to correct for confounding has led to misreporting in the media and retraction of scientific studies. You don’t want to be in that situation. So, always consider and check for confounding among the variables in your dataset. Confounders are variables that affect both your dependent and independent variables 8.0.2 Multiple Linear Regression There are ways to effectively handle confounders within an analysis. Confounders can be included in your linear regression model. When included, the analysis takes into account the fact that these variables are confounders and carries out the regression, removing the effect of the confounding variable from the estimates calculated for the variable of interest. This type of analysis is known as multiple linear regression, and the general format is: lm(dependent_variable ~ independent_variable + confounder , data = dataset). As a simple example, let’s return to the mtcars dataset, which we’ve worked with before. In this dataset, we have data from 32 automobiles, including their weight (wt), miles per gallon (mpg), and Engine (vs, where 0 is “V-shaped” and 1 is “straight”). Suppose we were interested in inferring the mpg a car would get based on its weight. We’d first look at the relationship graphically: ## take a look at scatterplot ggplot(mtcars, aes(wt, mpg)) + geom_point() scatterplot of wt and mpg from mtcars From the scatterplot, the relationship looks approximately linear and the variance looks constant. Thus, we could model this using linear regression: ## model the data without confounder fit &lt;- lm(mpg ~ wt, data = mtcars) tidy(fit) mtcars linear regression output From this analysis, we would infer that for every increase 1000 lbs more a car weighs, it gets 5.34 miles less per gallon. However, we know that the weight of a car doesn’t necessarily tell the whole story. The type of engine in the car likely affects both the weight of the car and the miles per gallon the car gets. Graphically, we could see if this were the case by looking at these scatterplots: ## look at the difference in relationship ## between Engine types ggplot(mtcars, aes(wt, mpg)) + geom_point() + facet_wrap(~vs) Scaterplot faceting for engine type From this plot, we can see that V-shaped engines (vs= 0), tend to be heavier and get fewer miles per gallon while straight engines (vs = 1) tend to weigh less and get more miles per gallon. Importantly, however, we see that a car that weighs 3000 points (wt = 3) and has a V-Shaped engine (vs = 0) gets fewer miles per gallon than a car of the same weight with a straight engine (vs = 1), suggesting that simply modeling a linear relationship between weight and mpg is not appropriate. Let’s then model the data, taking this confounding into account: ## include engine (vs) as a confounder fit &lt;- lm(mpg ~ wt + vs, data = mtcars) tidy(fit) confounding model taken into account Here, we get a more accurate picture of what’s going on. Interpreting multiple regression models is slightly more complicated since there are more variables; however, we’ll practice how to do so now. The best way to interpret the coefficients in a multiple linear regression model is to focus on a single variable of interest and hold all other variables constant. For instance, we’ll focus on weight (wt) while holding (vs) constant to interpret. This means that for a V-shaped engine, we expect to see a 4.44 miles per gallon decrease for every 1000 lb increase in weight. We can similarly interpret the coefficients by focusing on the engines (vs). For example, for two cars that weigh the same, we’d expect a straight engine (vs = 1) to get 3.5 more miles per gallon than a V-Shaped engine (vs= 0). confounding model taken into account Finally, we’ll point out that the p-value for wt decreased in this model relative to the model where we didn’t account for confounding. This is because the model was not initially taking into account the engine difference. Sometimes when confounders are accounted for, your variable of interest will become more significant; however, frequently, the p-value will increase, and that’s OK. What’s important is that the data are most appropriately modeled. 8.0.3 Correlation is not Causation You’ve likely heard someone say before that “correlation is not causation,” and it’s true! In fact, there are whole websites dedicated to this concept. Let’s make sure we know exactly what that means before moving on. In the plot you see here, as the divorce rate in Maine decreases, so does per capita consumption of margarine. These two lines are clearly correlated; however, there isn’t really a strong (or any) argument to say that one caused the other. Thus, just because you see two things with the same trend does not mean that one caused the other. These are simply spurious correlations – things that trend together by chance. Always keep this in mind when you’re doing inferential analysis, and be sure that you never draw causal claims when all you have are associations. Correlation does not equal causation In fact, one could argue that the only time you can make causal claims are when you have carried out a randomized experiment. Randomized experiments are studies that are designed and carried out by randomly assigning certain subjects to one treatment and the rest of the individuals to another treatment. The treatment is then applied and the results are then analyzed. In the case of a randomized experiment, causal claims can start to be made. Short of this, however, be careful with the language you choose and do not overstate your findings. 8.0.4 Beyond Linear Regression While we’ve focused on linear regression in this lesson on inference, linear regression isn’t the only analytical approach out there. However, it is arguably the most commonly used. And, beyond that, there are many statistical tests and approaches that are slight variations on linear regression, so having a solid foundation and understanding of linear regression makes understanding these other tests and approaches much simpler. For example, what if you didn’t want to measure the linear relationship between two variables, but instead wanted to know whether or not the average observed is different from expectation… 8.0.5 Mean different from expectation? To answer a question like this, let’s consider the case where you’re interested in analyzing data about a single numeric variable. If you were doing descriptive statistics on this dataset, you’d likely calculate the mean for that variable. But, what if, in addition to knowing the mean, you wanted to know if the values in that variable were all within the bounds of normal variation. You could calculate that using inferential data analysis. You could use the data you have to infer whether or not the data are within the expected bounds. For example, let’s say you had a dataset that included the number of ounces actually included in 100 cans of a soft drink. You’d expect that each can have exactly 12 oz of liquid; however, there is some variation in the process. So, let’s test whether or not you’re consistently getting shorted on the amount of liquid in your can. In fact, let’s go ahead and generate the dataset ourselves! ## generate the dataset set.seed(34) soda_ounces &lt;- rnorm(100, mean = 12, sd = 0.04) head(soda_ounces) In this code, we’re specifying that we want to take a random draw of 100 different values (representing our 100 cans of soft drink), where the mean is 12 (representing the 12 ounces of soda expected to be within each can), and allowing for some variation (we’ve set the standard deviation to be 0.04). output looking at soda_ounces dataset We can see that the values are approximately, but not always exactly equal to the expected 12 ounces. 8.0.5.1 Testing mean difference from expectation in R To make an inference as to whether or not we’re consistently getting shorted, we’re going to use this sample of 100 cans. Note that we’re using this sample of cans to infer something about all cans of this soft drink, since we aren’t able to measure the number of ounces in all cans of the soft drink generated. To carry out this statistical test, we’ll use a t-test. Wait, we haven’t talked about that statistical test yet. So, let’s take a quick detour to discuss t-tests and how they relate to linear regression. 8.0.5.2 Detour: Linear Regression &amp; t-tests R has a built in t-test function: t.test(). However, I mentioned earlier that many statistical tests are simply extension of linear regression. In fact, a t-test is simply a linear model where we specify to only fit an intercept (where the data crosses the y-axis). In other words, this specifies to calculate the mean…which is exactly what we’re looking to do here with our t-test! We’ll compare these two approaches below. However, before we can do so, we have to ensure that the data follow a normal distribution, since this is the primary assumption of the t-test. library(ggplot2) ## check for normality ggplot(as.data.frame(soda_ounces))+ geom_histogram(aes(soda_ounces), bins = 10) Here, we see that the data are approximately normally distributed, allowing for a t-test to be used. histogram of soda_ounces A t-test will check whether the observed ounces differs from the expected mean (12 oz). As mentioned above, to run a t-test in R, most people use the built-in function: t.test() ## carry out t-test t.test(soda_ounces, mu = 12) t.test() output In the output from this function, we’ll focus on the 95 percent confidence interval. Confidence Intervals provide the range of values likely to contain the unknown population parameter. Here, the population parameter we’re interested in is the mean. Thus, the 95% Confidence Intervals provides us the range where, upon repeated sampling, the calculated mean would fall 95 percent of the time. More specifically, if the 95 percent confidence interval contains the expected mean (12 oz), then we can be confident that the company is not shorting us on the amount of liquid they’re putting into each can. Here, since 12 is between 11.99187 and 12.00754, we can see that the amounts in the 100 sampled cans are within the expected variation. We could infer from this sample that the population of all cans of this soft drink are likely to have an appropriate amount of liquid in the cans. However, as mentioned previously, t-tests are an extension of linear regression. We could also look to see whether or not the cans had the expected average of 12 oz in the data collected using lm(). # from linear regression regression_output &lt;- lm(soda_ounces ~ 1) # calculate confidence interval confint(regression_output) comparing t.test() and lm() output Note that the confidence interval is the exact same here using lm() as above when we used t.test()! We bring this up not to confuse you, but to guide you away from trying to memorize each individual statistical test and instead understand how they relate to one another. 8.0.6 More Statistical Tests Now that you’ve seen how to measure the linear relationship between variables (linear regression) and how to determine if the mean of a dataset differs from expectation (t-test), it’s important to know that you can ask lots of different questions using extensions of linear regression. These have been nicely summarized by Jonas Kristoffer Lindelov in is blog post Common statistical tests are linear models (or: how to teach stats). We’ve included the table summarizing his post here, but we recommend you check it out and the examples included within it carefully! Common statistical tests are linear models 8.0.7 Summary In these few lessons on inference, we have covered a lot. We started off discussing that inferential data analysis is required since we often cannot take measurements from everyone in our population of interest. Thus, we take representative samples and estimate what the measure is in the larger population. Since this is an estimate, there is always some measure of uncertainty surrounding this estimate. We talked about how inferences are drawn using a single numeric variable. We talked in detail about simple linear regression and touched on multiple linear regression, with a brief discussion on confounding. We discussed that linear regression helps describe the relationship between two numeric variables (trees example) and that multiple linear regression helps account for confounding in an analysis (mtcars example). We discussed t-tests (the soda example), including how t-tests (and many other common statistical tests!) are variations on linear regression. We’ll end this lesson by reiterating that being able to draw conclusions and clear interpretations of your analyses is critical and that you should never draw causal conclusions when you have associations. 8.0.8 Additional Resources Common statistical tests are linear models (or: how to teach stats), blog post by Jonas Kristoffer Lindelov 8.0.9 Slides and Video Automated Video Slides "],["inference-examples.html", "Chapter 9 Inference: Examples", " Chapter 9 Inference: Examples In the last lesson, we went through a number of the basics of simple linear regression, discussed confounding, introduced multiple linear regression, and discussed analytical approaches built off of linear regression. Through that lesson and in the swirl modules in the quiz, we went through a few simple examples; however, since linear regression is the foundation of tons of work being done in the real world, we wanted to walk through two additional examples - which we’ll call case studies - in this lesson. This lesson will help you: See how linear regression can be used Understand the types of questions where it’s applicable Learn to interpret results you find in the world Analyze your own data in the future 9.0.1 Introduction Data scientists, statisticians, social scientists, economists, and natural scientists are all related to one another in that they all ask questions about the world and use data to answer them. In this lesson, through three case studies, we’ll happen focus on questions that economists have asked in recent years, but these approaches are used by all types of people analyzing data and answering questions. In this lesson, we’ll discuss the data economists have collected to answer their questions of interest, the analyses they’ve carried out, and their results. 9.0.2 Case Study #1: The Effects of Watching Sesame Street on Educational Outcomes In an earlier lesson we mentioned that when carrying out inferential analyses, analysts are taking a small sample of data to say something about what would happen if we collected more data, say, from the entire population. The example question we introduced was trying to measure and understand the effect of Sesame Street on children’s learning. To better understand this relationship, we proposed that an analyst could set out to answer the question: “Is there a relationship between watching Sesame Street and test scores among children?” In fact, many economists have studied exactly this! In our first case study, we’ll walk through a recent analysis looking at the effects of Sesame Street on educational outcomes. Eary Childhood Education by Television 9.0.2.1 Question The general question the economists we’ll be talking about today (Melissa S. Kearney and Phillip B. Levine) set out to answer was: “Does exposure to Sesame Street improve educational and labor market outcomes?” Now, this group was not the first to study a question involving Sesame Street and educational outcomes, but they did build upon the work of others’ in their analysis. This means that they had to figure out what specific question they were going to ask to add something new to the discussion. Rather than looking at a small sample of children (as others had done previously), the economists we’ll be discussing here set out to answer the question for a larger group: How was an entire generation of children impacted by having access to Sesame Street? While this question is getting more specific, the actual questions they’d ask was driven by the data to which they had access. 9.0.2.2 Data When asking questions about populations across an entire country on data that were generated more than 50 years ago (Sesame Street debuted in 1969), often the data you need are not all in a single location. Thus, to have measurements on availability of Sesame Street as well as educational and labor outcomes, these economists had to use a number of different data sources: Census Data - 1980, 1990, and 2000 Census data 1980 High School and Beyond Survey Broadcast exposure data - when Sesame Street was broadcast, only certain parts of the country had access to the channels that broadcast Sesame Street Among these data, the economists would be most interested in the following variables: 1. Sesame Street Coverage Rates (or, how available was Sesame Street in the county in 1969) 2. Educational outcome 3. Labor market outcomes 4. Policy changes (indicator for whether Food Stamp program had been introduced and Head Start Program expenditures had increased) 5. Individual demographic information - race/ethnicity, age, and socioeconomic status (SES) Sesame Street Coverage Rates 9.0.2.3 Analysis These economists used two primary regression models to analyze these data. It was a multiple regression model; however, admittedly, it is a tad more complex than those models we’ve considered so far. Now, fundamentally it is just a multiple regression model where the effect of Sesame Street coverage on outcome (educational or labor market) is being measured while controlling for policy changes and demographic factors. However, I will note that the way in which policy and demographic factors were included goes beyond the scope of this lesson. 9.0.2.4 Results Nevertheless, we can still interpret the results from this analysis! The results, as they often are for inferential analyses, are presented in the form of a table. Before presenting the results of the table, the economists first demonstrate (using a figure) that before 1969 when Sesame Street was introduced, there was no difference in students at grade level between areas that would go on to have Sesame Street coverage and those that would not. However, as time goes on, their analysis demonstrates an increase in students at grade level in areas where Sesame Street was available. Sesame Street availability correlated with areas where more students were at grade level after 1969 Table 4 from this paper then summarizes the results looking at the impact Sesame Street coverage (meaning how available Sesame Street was in an area) has on educational outcomes. The authors present the effect sizes and standard errors in parentheses. To intercept these, values greater than 0 have a positive relationship, less than zero a negative relationship. And, the smaller the standard error, the more confident we are in the estimate. Table 4 summarizes results looking at impact of Sesame Street on Educational Outcomes For example, if we just focus on the first row measuring the aggregate effect, we see that the effect size is 0.105. Being greater than 0, we see that there is some positive effect. This means that the more Sesame Street was available, the better the educational outcomes. The standard error here is 0.041, a value close to but not exactly zero, suggesting that we’re relatively confident that this effect is truly close to 0.105. Sesame Street Availability is associated with an increase in educational attainment To interpret what an effect size of 0.105 means in the context of this analysis, a 1 point increase in coverage rates would lead to a 0.105 percentage point increase in the rate of grade-for-age status (their measure of educational attainment). The economists who carried out the study, however, point out that a 1 point increase in coverage rates is not typical. Rather, if a child were to move from a low coverage area to a strong coverage area, this would actually be a 30 point increase in coverage rates. This means that the effect for moving from a low coverage to a high coverage area would actually result in a 3.2% (0.3 * 0.105 = 0.032) increase in the rate of grade-for-age status. The authors also stratified their results by gender and race. By comparing the effect sizes, we can see that the effect is larger in males than it is in females and in black children relative to white children. The results are strongest in males and black children To demonstrate that this effect has anything to do with Sesame Street, the authors carried out the same analysis before Sesame Street was released. In doing this, the economists are looking for effect sizes close to zero, suggesting that the availability of Sesame Street in 1969 is truly playing a role in improving educational outcomes. This is what we call a negative control, meaning they’re doing this analysis expecting no effect (effect sizes near zero). The authors present their results from this null analysis in a table again. Before Sesame Street availability, coverage did not demonstrate a difference in educational attainment 9.0.2.5 Conclusion In this work, the economists set out to understand at the population level how the availability of Sesame Street affects the educational outcomes of children across the country. They had to figure out how to measure Sesame Street availability, educational outcomes, policy changes, and demographic information before being able to use linear regression and answer their question. Once they determined that they were able to measure all of their needed variables at the county level, they carried out their regression and demonstrated that when Sesame Street was available, a higher percentage students started performing at their grade level - a positive relationship between Sesame Street availability and educational attainment! 9.0.3 Case Study #2: Effects of Religion on Happiness Economists have long been interested in measuring happiness and understanding happiness. Sometimes they’re interested in understanding a single person’s happiness and what contributes to that; however, more often they’re looking at populations - cities, states, or whole countries and trying to understand why certain populations are happier than others. A recent analysis in the American Economic Review set out to study the relationship between religiosity and happiness. 9.0.3.1 Question The general question the economists (Deaton and Stone) started with was “Is religion good for you?” However, as we’ve discussed in previous courses, in order to analyze this question using data, we really need a more specific question, one where we know what we’re measuring in order to answer this question. To answer this question, the researchers knew they needed, at a minimum, information on people’s religiosity as well as reports of how happy they are (also known as their “well-being”). 9.0.3.2 Data The authors used data from two different sources: Gallup Healthways Well-being Index survey (N=1,000 randomly-selected Americans surveyed daily since January 2008) Gallup World Poll (N=1,000+ people from 160 different countries) Religiosity People responded either yes or no to the question “Is religion an important part of your daily life?” Well-being ladder - people rate their lives on a scale from 0 (worst possible life) to 10 (best possible life) happiness - people respond either yes or no to the question “Did you experience a lot of happiness yesterday?” The economists running this study had to determine what they were going to measure and where those data were going to come from before ever being able to answer their question. They settled on two different measures of well-being and a single measure of religiosity. Note that these are survey data, so we’re relying on individuals reporting information accurately, something that does not always happen with surveys…and certainly something to always keep in mind when interpreting results. 9.0.3.3 Analysis To analyze these data, the researchers had to decide which variables to use (meaning what to measure, discussed above) and how to analyze it. They chose to utilize linear regression, with religiosity as the independent variable and well-being as the dependent variable. The authors were particularly interested in understanding a paradox. The authors know that individuals who are religious tend to have higher self-reported well-being; however, regions (states or countries) that are more religious tend to have lower well-being as a population. The individual effect is completely opposite to what we see at the population level. While this paradox has been reported before, these authors wanted to look in this much larger dataset to determine the effect of religion on your health. 9.0.3.4 Results The authors carried out a number of linear regressions. At times it was simple linear regression (i.e. looking at the relationship between individuals religiosity and self-reported well-being) and sometimes it involved multiple linear regression (the same model, but accounting for differences in income). They reported their findings in a table where they include the beta coefficient, standard error (SE), and number of observations used to calculate the statistics. They do this for both the ladder measurement of well-being as well as the happiness measure and reported all of these findings in a single table. Table 2 summarizes relationship between self-reported well-being and religiosity 9.0.3.5 Interpretation To understand the answer to the question “Is religion good for you?,” readers have to know how to interpret this type of table. 9.0.3.5.1 Religiosity and Ladder-happiness in the USA Let’s first try to interpret the relationship between people’s religiosity (independent variable) how people rank their lives (the dependent variable fist focusing on ladder score - a measure from 0 to 10) among individuals in the USA. Religious individuals in the USA have higher self-reported well-being Focusing on the pink box here, we see that person-level data has a positive beta coefficient, indicating a positive relationship between religiosity and well-being. This means that religious people (on average) tend to have higher ladder measure of well-being than non-religious individuals. The second row here demonstrates that when accounting for income (with multiple regression), this result remains and in fact gets stronger (0.263 &gt; 0.248). We can also see that these results come from analyzing individual data from more than 1 million individuals(the Observations column) and that the error around this estimate is quite small (SE column). Moving to the third row here, we see that at the state level (rather than the individual level), there is a slight, negative relationship between religiosity and well-being. This result suggests that the more religious a state is, the less happy they are as a state. However, once accounting for income, we see that this negative relationship goes away, suggesting that income, rather than religiosity, drives this effect. 9.0.3.5.2 Religiosity and Yesterday’s happiness in the USA Now, remember that well-being was measured in more than one way. We can now look at the USA-level results where there were only two possible options - yes or no to the question: “Did you experience a lot of happiness yesterday?” Little or no effect of religiosity on yesterday’s happiness in USA Here the results are all positive, but also very close to zero, suggesting that the effect of religiosity on yesterday’s measure of happiness is quite small. 9.0.3.5.3 Worldwide religiosity and happiness One benefit of this study was that the researchers had measurements from many people worldwide, so they were able to study the effect of religiosity on well-being across countries. Religious individuals in the USA have higher self-reported well-being Again, we turn to the table to look at this relationship. Looking at the beta coefficients, we see that the results suggest that more religious countries tend to have lower well-being measurements (measured by the negative beta coefficient); however, once income was accounted for, this result demonstrated that more religious countries (when accounting for income) tend to have higher measures of ladder well-being. The same general trend held across countries when looking at the effects of religiosity on yesterday’s happiness. 9.0.3.6 Conclusion The economists of this study discuss a number of confusing findings related to happiness and suggest possible explanations of these results. They use regression to understand the effects of the independent variable (religiosity or income) on the dependent variable (well-being / happiness). However, as with all studies, one set of analyses does not answer all the questions on the subject - additional analyses and experiments would be needed to understand these results fully. 9.0.4 Summary In this lesson we walked through two real-life examples of inference being used to answer questions. In the first we looked at the relationship between the availability of Sesame Street and educational attainment. In the second, we looked at the effect of religiosity on well-being. Both studies used regression in their inferential analyses to answer their questions of interest. We discussed effect sizes (beta coefficients) and their interpretation in the previous lesson as well as this lesson. Understanding regression will get you a great start in understanding inferential (and even predictive!) analysis. 9.0.5 Additional Resources Early Childhood Education by Television: Lessons from Sesame Street, by Melissa S. Kearney and Phillip B. Levine Why Kids Who Watched Sesame Street Did Better In School, article in Quartz by Annabelle Timsit Two Happiness Puzzles, by Angus Deaton and Arthur A. Stone 9.0.6 Slides and Video Automated Video Slides "],["inference-practice.html", "Chapter 10 Inference: Practice", " Chapter 10 Inference: Practice Often when we’re learning new concepts we get focused on one part of the process (getting the code to run!) and lose sight of the big picture (why are we doing this analysis again?). In the last few lessons we’ve discussed a lot. We’ve discussed regression conceptually, gone through examples of running simple and multiple regression, and saw a few examples of regression in the real world. Now it’s time to make sure we’re able to carry out our own regression analysis from start to finish! The goal of this lesson is to ask an inference-focused question and use data to answer that question. In this we’ll explore a dataset (EDA!) and fit a regression model (using linear regression!) Be sure that you’re carrying out the analysis on your own throughout this lesson to understand what’s going on here and because you’ll need the objects created in this lesson to complete the quiz at the end. Feel free to copy the code but make sure you’re understanding not only the code but also the output. Getting code to run is half the battle, but we need to be sure we understand the output of the code for it to mean anything! 10.0.1 The Question Health Care in the United States is a complicated mess of confusing coverage policies, costly procedures, and hard-to-understand insurance companies. But, understanding this system and how it affects health in the United States is important! In this lesson, we’re going to walk through an example using a real dataset to better understand healthcare coverage and spending across the United States. The specific question we’re setting out to answer is: What is the relationship between healthcare coverage and healthcare spending in the United States? 10.0.2 The Data The data and motivation for this question come from an awesome project called Open Case Studies. This project provides datasets and tutorials that use real data to teach data-related concepts. The specific case study we’re using in this lesson can be found here. We’re going to walk through this analysis step by step, using a number of their approaches and materials. But, you can certainly find more information in the case study directly! To get started working with these data in RStudio Cloud, we’ll need to first read the data in using read_csv and the URL directly. # uncomment if packages not yet installed # install.packages(&quot;readr&quot;) library(readr) # read in coverage data coverage &lt;- read_csv(&quot;https://raw.githubusercontent.com/opencasestudies/ocs-healthexpenditure/master/data/KFF/healthcare-coverage.csv&quot;, skip = 2, n_max = 52) # read in spending data spending &lt;- read_csv(&quot;https://raw.githubusercontent.com/opencasestudies/ocs-healthexpenditure/master/data/KFF/healthcare-spending.csv&quot;, skip = 2, n_max = 52) Note in the code here there are a few additional arguments that have to be included. We use skip = 2 to skip the first two rows in the file and set n_max = 52. This is something we haven’t seen previously, but n_max specifies the maximum number of records to read in. Here, we’re specifying to find the location where the column Location is Notes and then stop the row before that. This will ignore Notes included at the end of the file. You can see the notes included if you scroll to the end of one of these files on GitHub. Recognizing that these steps have to be done is part of carrying out a project. It’s your job as the analyst to know and understand the dataset inside out and backwards. Familiarizing yourself with the many arguments in functions like read_csv() from readr (use ?readr::read_csv to see them) will help you keep in mind the things to look out for when reading datasets into R. 10.0.3 Descriptive Analysis The first thing you’ll want to do is get a sense of what data are included in each dataset to familiarize yourself with the information included. Remember that the glimpse() function can be very helpful here! # install.packages(&quot;dplyr&quot;) library(dplyr) glimpse(coverage) glimpse() gives us a look at what’s included in our coverage dataset Looking at the output here, we see that there are 52 observations and 29 variables being measured. We see that the observations are the states in the US (likely including Washington, DC) and one for the US overall. We then see that a number of pieces of data (variables) have been collected over four years (2013-2016). We also get an idea of what types of data are stored in our dataset. We see lots of numeric variable (dbl) and a few strings (chr). But wait…should those strings actually be strings? It looks like there’s numeric information stored in those variables (2013__Other Public, for example). It looks like the way missing data has been specified in these data (“N/A”) is causing some issues… R is expecting “NA” by default…so we’ll have to go back and specify for R to look for “N/A” as well in just a second. But first, let’s see if we have the same issue in the spending dataset. glimpse(spending) glimpse() demonstrates we have the same number of observations in the spending dataset When we look at the glimpse() output for spending, we see that we again have the same number of observations (N=52) but here we have 25 variables summarizing total health spending from 1991-2014. So, right off the bat we know that the only two years we have overlapping information are 2013 and 2014. Something to keep in mind! Also, we see that all observations (aside from Location) are numeric (dbl), so it doesn’t appear we have any “N/A” issues here. Before we go any further, let’s address those “N/A” values. If you looked at the read_csv documentation, you’re aware that na is an argument to that function. So, let’s read these data in once again, but specify that “N/A” is how missing data has been specified in the coverage dataset. # read in coverage data # consider how N/A is specified coverage &lt;- read_csv(&quot;https://raw.githubusercontent.com/opencasestudies/ocs-healthexpenditure/master/data/KFF/healthcare-coverage.csv&quot;, skip = 2, n_max = 52, na = &quot;N/A&quot;) Now, after specifying how to handle missing values, when we use glimpse(coverage) we see that numeric data are all of type dbl as we expected! It’s important to always take a look at the output you generate. If we hadn’t looked at our glimpse() output carefully, we would have missed this and our variable would have been of the wrong type! So, a friendly reminder to always look at your output. Now that our datasets are in good order and we have an idea of what information is included in them generally, remember that the skimr package is also very helpful for getting a quick and deep understanding of your data. If we use the skim() function from this package on each dataset, we can learn more information about the data we have. In this output, we get information not just about how many observations we have and what variables we have information about and their types, we also start to get an understanding of the typical values (mean, median) and range (min, max) for the data in our dataset. # install.packages(&quot;skimr&quot;) library(skimr) skim(coverage) skim() provides a ton of helpful information For example, in our coverage dataset, we quickly see that most variables have complete information, but there is missing information in the Other Public variables across the years. We also know that the average (mean) for Total has remained constant (1.2 x 10^7) for the years included in this dataset. skim(spending) skim() summarizes the spending dataset We can similarly see at a glance that there is no missing information in the spending dataset. However, unlike in the coverage dataset, here, with data from more years we can see spending has increased over time. Specifically, the average in 1991 was 25996.06 and 98570.17 by 2014. Using glimpse() and skim() we get an understanding of what information is included in our dataset as well as what typical values are and what data are missing. What we don’t get here is variability across states, whether one state spends more than others, and what the relationship between spending and coverage is. That’s what EDA is for! 10.0.4 EDA With an understanding of the data we have, we’re now able to start understanding the dataset deeply through exploratory data analysis (EDA). To ensure that we’re going in the right direction, it’s important to keep our question of interest in mind. We ultimately want to measure the relationship between healthcare coverage and healthcare spending in the United States. At this point, these data are stored in two different dataframes. To explore their relationship, we’ll want to wrangle them into a single dataframe before continuing forward. 10.0.4.1 Data Wrangling To decide how to best do this we have to consider what type of join we need here. Remember that there are a number of common joins: left_join, left_join, inner_join, and full_join. (Refer back to the Getting Data course if you need a review on these). We have to decide for our purposes which join is most appropriate. We noted earlier that coverage and spending only overlap by two years (2013 and 2014). If we only want to analyze the data where we have information from both years, we’ll need to use inner_join. Additionally, we’ll need to be able to tell R what columns to join on. Unfortunately, at this point our data are not in the tidiest of formats. Remember, tidy data has the following properties: Each variable in a column. Each observation in a row. Each type of observational unit forms a table Neither dataset follows these principles. We’ll have to wrangle our data to get them into the appropriate format before we can join them together! To do this, remember that tidyr is a really helpful package for wrangling and reshaping data. (Refer back to the Data Tidying course if you need a refresher on reshaping data.) Currently, the coverage dataset is in a wide format, but we want it in a long format where each Location is a row and we have information about the year, type and total coverage in the columns. To accomplish this, we’ll use gather()! # install.packages(&quot;tidyr&quot;) library(tidyr) coverage &lt;- gather(coverage, &quot;year_type&quot;, &quot;tot_coverage&quot;, -Location) coverage gather() takes our “wide” coverage dataset to “long” We’ll want to do the same to our spending dataset: spending &lt;- gather(spending, &quot;year&quot;, &quot;tot_spending&quot;, -Location) gather() takes our spending dataset from “wide” to “long” We’re making progress! Our data are in a tidy format now; however, you may have noticed that information about year is tied up with other information. We’ll want to separate that out. Fortunately, tidyr has the incredibly helpful separate() function! We’ll use that to separate out year information so that we can use that in our join! Note that here we’re including convert = TRUE so that the resulting year columns is of type int. coverage &lt;- coverage %&gt;% separate(year_type, sep=&quot;__&quot;, into=c(&quot;year&quot;, &quot;type&quot;), convert = TRUE) separate() to separate column contents And, just like that, we’ve got year in its own column! Now, let’s repeat that for our spending dataset! We’ve also included the select() function to remove the name column. This includes the same information (“Total Health Spending”) for every row, and thus we can get rid of it. We use the - operator to remove a column from our dataset: spending &lt;- spending %&gt;% separate(year, sep=&quot;__&quot;, into=c(&quot;year&quot;, &quot;name&quot;), convert = TRUE) %&gt;% select(-name) spending spending dataset now has appropriate tidy data format and appropriate columns 10.0.4.2 Join: inner_join The datasets are now in a tidy format with each row containing an observation (different state or location) and each column contains a variable. We can also now join our data so that we’re only looking at coverage and spending data from the years we have both pieces of information. We’ll join using the Location and year columns. hc &lt;- inner_join(coverage, spending, by = c(&quot;Location&quot;, &quot;year&quot;)) hc inner_join() gets information in a single dataframe We now have a dataset with 728 observations where each row contains coverage and spending information for a particular Location in a particular year. Okay, before we move on, you may think to yourself…“OK, I know that different states have different populations. That’s probably important here.” If you thought that to yourself, you’re right! And, you’re also in luck. That information is hiding within the type variable. The Total information here includes the total population. We can include that as a column in our dataset! pop &lt;- hc %&gt;% filter(type == &quot;Total&quot;) %&gt;% select(Location, year, population = tot_coverage) pop extract population information The population column now includes the population for each Location in each year in our dataset! Let’s add that information to our hc dataset by first removing all rows where type is “Total” (using filter()) and then by combining the datasets (using left_join). hc &lt;- hc %&gt;% filter(type != &quot;Total&quot;) %&gt;% left_join(pop, by = c(&quot;Location&quot;, &quot;year&quot;)) hc with population information included in hc With population information, we will want to calculate spending per capita and the proportion of people covered in each state. Spending per capita calculates the spending per person. Having these values allow us to compare values across states with different populations. We use mutate to accomplish this: hc &lt;- hc %&gt;% mutate(spending_capita = (tot_spending*1e6) / population, prop_coverage = tot_coverage / population) calculated spending_capita using population information Okay, we’re almost fully wrangled! But, before we move on, we know what Location is in each row; however, it could be helpful to have the two-letter abbreviation and region of the United States in this dataset, so let’s add that in. And, we’re in luck! There’s an R package (datasets) with just that information in it that you can use! Let’s use the data in that dataset to add that information to our hc dataframe! library(datasets) data(state) Note that this includes a number of objects that start state.. We’ll add the two-letter abbreviation (state.abb) and region (state.region) to our hc dataset using mutate(). hc &lt;- hc %&gt;% mutate(abb = state.abb[match(Location, state.name)], region = state.region[match(Location, state.name)]) hc abb and region information added to hc dataset Note that this adds two additional columns (abb and region) to ourhc` dataframe. Also, note that where there is not state information (“United States” and “District of Columbia”), these columns have NA, as we would expect! And just like that, we have all the information we’ll need! It took some work to get there, and it may not always be obvious exactly what step has to be done next, but by keeping the tidy data principles in mind and using the tools we’ve learned, we’re ready to explore these data further! 10.0.4.3 Exploratory Visualizations With our dataset in the correct format, let’s take a look using ggplot2 to better understand our data! We’re ultimately interested in understanding the relationship between coverage and spending across states, so we’ll want to be sure to look at the distributions of each of those variables. But, we’ll want to use the ones adjusted for population as those are comparable across states. We’ll also want to remove the rows that are not states (US, and Washington DC). And, let’s just look at Employer-supplied health coverage. # focus on 50 states hc &lt;- hc %&gt;% filter(Location != &quot;District of Columbia&quot;, Location != &quot;United States&quot;) # just employer hc_employer &lt;- hc %&gt;% filter(type == &quot;Employer&quot;) Now, we’re ready to look at some histograms! As a reminder, we won’t spend a ton of time making these perfect visualizations because during EDA the goal is not to communicate our results perfectly, but rather just to inform us (the analysts) about the data. Later on if you were to present these in a meeting or at a conference, you would want to spend time making sure they’re explanatory (rather than just exploratory) visualizations. # install.packages(&quot;ggplot2&quot;) library(ggplot2) ggplot(hc_employer, aes(prop_coverage)) + geom_histogram() prop_coverage shows a bimodal distribution The proportion covered shows a distribution where coverage ranges from around 40% to 60% across the 50 states in 2013. ggplot(hc_employer, aes(spending_capita)) + geom_histogram() spending_capita shows is approximately Normal Unlike the proportion covered, spending per capita is approximately Normal with a median somewhere right below $8,000. We do see a few outliers. We’ll want to see which states these are that have much lower and much higher than average spending per capita. We’re ultimately interested in understanding the relationship between these two variables, so let’s take a look at that with a scatterplot: ggplot(hc_employer, aes(x = spending_capita, y = prop_coverage)) + geom_point() + xlab(&quot;spending per capita&quot;) + ylab(&quot;coverage proportion&quot;) relationship between spending and coverage At a glance, there appears to be a linear relationship between these two variables where the more spending per person in a state, the higher proportion of the population has health care supplied by the employer. To fully understand this relationship, let’s explore it with linear regression! 10.0.5 Linear Regression To answer our question: What is the relationship between healthcare coverage and healthcare spending in the United States? we’ll model this relationship using linear regression. Visually, this trend-line would look as follows: hc_employer %&gt;% filter(year == &quot;2013&quot;) %&gt;% ggplot(aes(x = spending_capita, y = prop_coverage)) + geom_point() + xlab(&quot;spending per capita&quot;) + ylab(&quot;coverage proportion&quot;) + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) visualizing the linear relationship with ggplot2 In addition to looking at this visually, we also want to understand the regression model and take a look at model diagnostics. To do so we’ll use lm and tidy. (tidy is from the broom package) # install.packages(&quot;broom&quot;) library(broom) hc_2013 &lt;- hc_employer %&gt;% filter(year == &quot;2013&quot;) fit &lt;- lm(prop_coverage ~ spending_capita, data = hc_2013) tidy(fit) broom output for 2013 linear regression model Here we see that across states in the US in 2013, for every 1 dollar more spent on healthcare, the proportion of individuals covered increase by 0.000019 (the estimate column in the tidy() output). While this number is quite small, a 1 USD increase in health care spending is probably not the right unit to consider. We would likely want to consider a $1000 increase in spending. For example, to interpret this model, we’d say that for every $1000 more in health care spending, we see a 1.9% increase in the population covered across the US. (We get this number by multiplying the coefficient 0.000019 * 1000 and then converting from proportion to percentage by multiplying by 100.) But, remember that each point here represents a state! And, states that are in the same region of the US may be clustered. The region a state is in may affect health care spending and the proportion of the population covered! A confounder! Visually, let’s take a look by labeling each point with the two-letter state abbreviation and coloring the points based on the region of the US where the state is located. ggplot(hc_2013, aes(x = spending_capita, y = prop_coverage, color = region)) + geom_point() + xlab(&quot;spending per capita&quot;) + ylab(&quot;coverage proportion&quot;) + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) + geom_text(aes(label=abb), nudge_x = 150) states cluster based on geographic location In this plot we see that states tend to cluster. States in the Northeast tend to spend more on healthcare and have a higher proportion of the population covered, whereas the opposite is true in the South. This suggests that to understand the overall relationship, we likely want to consider region as a confounder in the analysis. fit2 &lt;- lm(prop_coverage ~ spending_capita + region, data = hc_2013) tidy(fit2) broom output for 2013 multiple regression model accounting for region as a confounder In this regression model, we see when we focus on the spending_capita row, that the estimate is much smaller (0.000006) and, when we look at the p.value column, that this result is likely insignificant (as we would see results this extreme by chance along 35.9% of the time). This suggests that regional differences are contributing largely to the relationship between spending per capita and the proportion of the population with health coverage. Without considering this confounder we would have come to the wrong conclusion! What we learned here was that there are regional differences across the country regarding how much is spent and how much of the population has health coverage! What we didn’t learn is that there is a significant relationship between spending and coverage. To see this visually, consider faceting by region: ggplot(hc_2013, aes(x = spending_capita, y = prop_coverage, color = region)) + geom_point() + xlab(&quot;spending per capita&quot;) + ylab(&quot;coverage proportion&quot;) + geom_smooth(method = &quot;lm&quot;, col = &quot;red&quot;) + geom_text(aes(label=abb), nudge_x = 150) + facet_wrap(~region) faceting by region demonstrates limited relationship between spending and proportion of population covered Here, we see pretty flat lines (particularly in the Northeast and the West), supporting the findings from our multiple regression model. 10.0.5.1 Model diagnostics Don’t forget that we have to check our model assumptions to determine whether linear regression was appropriate. To do this we can use the plot function on the output from fitting our model: par(mfrow = c(2,2)) plot(fit2) model diagnostics We’re looking here for: a horizontal red line in our Residuals vs. Fitted plot, demonstrating a linear relationship between variables. a horizontal line in our Scale-Location plot, demonstrating the data are homoscedastic, points falling along the dotted line in our QQ Plot, demonstrating the residuals are Normally distributed standardized residuals to fall between -2 and 2 in the Residuals vs. Leverage plot, suggesting there are no obvious outliers driving our regression results In our model diagnostics plot, we see that the first three bullet points are met, demonstrating that linear regression is appropriate; however, we see in the Residuals vs. Leverage plot that there are some outlier states in our analysis. Before finalizing our analysis, we would want to figure out which states these are and why their values are outliers in this dataset. Finally, note that we only looked at “Employer” type of coverage in 2013. Feel free to carry out similar analyses as we’ve done here to look at other types of coverage and/or for the year 2014 to be sure you fully understand these analyses. 10.0.6 Summary In this lesson, we’ve put a whole lot together. We walked through how to carry out a regression from start to finish. From asking the question, to reading the dataset in, to exploring the dataset, and to generating the model, there is a lot to do! Hopefully this has started to pull the previous few lessons all together and you’re getting more comfortable with what inference is, know how to carry it out in R, and most importantly understand how to interpret these analyses. 10.0.7 Additional Resources Open Case Studies, by Pei-Lun Kuo, Leah Jager, Margaret Taub, and Stephanie Hicks Health Expenditures Case Study Case Study on GitHub join cheatsheet, by Jenny Bryan DataTrail Getting Data Course DataTrail Data Tidying Course 10.0.8 Slides and Video Automated Video Slides "],["prediction-and-machine-learning.html", "Chapter 11 Prediction and Machine Learning", " Chapter 11 Prediction and Machine Learning In the last lesson, we discussed that inferential data analysis looks to learn something about a population by making inferences from a representative sample. While the goal in inference is to learn something about the population, when we’re talking about prediction, the focus is on the individual. The goal of predictive analysis and machine learning approaches is to train a model using data to make predictions about an individual. In other words, the goal of predictive analysis is to use data you have now to make predictions about future data. We spend a lot of time trying to predict things in daily life- the upcoming weather, the outcomes of sports events, and the outcomes of elections. We’ve previously mentioned Nate Silver at FiveThirtyEight, where they try and predict the outcomes of U.S. elections (and sporting events too!). Using historical polling data and trends and current polling, FiveThirtyEight builds models to predict the outcomes and the next US Presidential vote - and has been fairly accurate at doing so! FiveThirtyEight’s models accurately predicted the 2008 and 2012 elections and was widely considered an outlier in the 2016 US elections, as it was one of the few models to suggest Donald Trump had a chance of winning. Predicting the outcome of elections is a key example of predictive analysis, where historical data (data they have now) are used to predict something about the future. Basics of Predictive Analysis In this lesson we’ll walk through the important pieces of carrying out a predictive analysis, what considerations should be made when making predictions, discuss what machine learning is, and talk about how to assess accuracy within a predictive analysis. 11.0.1 What is Machine Learning? So far we’ve been discussing predictive analysis. But, you may have heard people on the news or in daily life talking about “machine learning.” The goal of machine learning is to build models (often referred to as algorithms) from the patterns in data that can be used for predictions in the future. For our purposes, it’s safe to argue that when doing predictive analysis, we’re actually doing machine learning. As such, we’ll use machine learning throughout the rest of this lesson. Here, machine learning refers to using the relationships within a dataset to build a model that can be used for prediction. That said, there is without a doubt an entire field of individuals dedicating themselves to machine learning. This lesson will just touch on the very basics within the field. 11.0.2 Machine Learning In order to make predictions for the future using data you have now, there are four general steps: Data Splitting - what data are you going to use to train your model? To tune your model? To test your model? Variable Selection - what variable(s) from the data you have now are you going to use to predict future outcomes? Model Selection - How are you going to model the data? Accuracy Assessment - How are you going to assess accuracy of your predictions? Basic Steps 11.0.3 Data Splitting For predictive analysis (or machine learning), you need data on which to train your model. These are the set of observations and corresponding variables that you’re going to use to build your predictive model. But, a predictive model is only worth something if in can predict accurately in a future dataset. Thus, often, in machine learning there are three datasets used to build a predictive model: train, tune, and test. 11.0.3.1 Train, Tune, Test Okay, admittedly, these are not the most common words to use for this process. Many people use train, validate, and test. However, almost as many people use train, test, and validate, as evidenced by this Twitter poll: Twitter poll As such, we’re mentioning those terms so that you’re familiar with them, but since machine learning people can’t agree on the order of the words, in this lesson, we’ve decided to go with more helpful terminology, as suggested by Carl de Boer: train, tune, and test. Train, Tune, Test 11.0.3.1.1 Train Training data are the data we described above. The data used to build your predictive model. These data are referred to as your training set. training data 11.0.3.1.2 Tune Before getting started, your original dataset is often split. Some (often 70%) of the observations in your dataset are used to train the model, while 30% are held out. This held-out set of observations from your original dataset are then used to improve (tune) the accuracy of model. These hold-out samples are used to see whether or not your predictive model accurately makes predictions in the set of samples not used to train the model. tuning data 11.0.3.1.3 Test Finally, an independent dataset – one that is not from the same experiment or source as the data used to train and tune your model are used to see whether or not your predictive model makes accurate predictions in a completely new dataset. Predictive models that can be generalized to and make accurate predictions in new datasets are the best predictive models. testing data 11.0.4 Variable Selection For predictive analysis to be worth anything, you have to be able to predict an outcome accurately with the data you have on hand. If all the data you have on hand are the heights of elephants in Asia, you’re likely not going to be able to predict the outcome of the next US election. Thus, the variables in the data you have on hand have to be related to the outcome you’re interested in predicting in some way (which is not the case for the heights of elephants and US elections). Instead, to predict US elections, you’d likely want some data on outcomes of previous elections, maybe some demographic information about the voting districts, and maybe some information about the ages or professions of the people voting. All of these variables are likely to be helpful in predicting the outcome in a future election, but which ones are actually predictive? All of them? Some of them? The process of deciding which variables to use for prediction is called variable selection. Variable Selection You ideally want to include the fewest variables in your model as possible. Only having a few variables in your model avoids you having to collect a ton of data or build a really complicated model. But, you want the model to be as accurate as possible in making predictions. Thus, there’s always a balance between minimizing the variables included (to only include the most predictive variables!) and maximizing your model’s predictive accuracy. In other words, like in inferential analysis, your ability to make accurate predictions is dependent on whether or not you have measurements on the right variables. If you aren’t measuring the right variables to predict an outcome, your predictions aren’t going to be accurate. Thus, variable selection, is incredibly important. All that said, there are machine learning approaches that carry out variable selection for you, using all the data to determine which variables in the dataset are most helpful for prediction. Nevertheless, whether you are deciding on the variables to include or the computer is deciding for you, variable selection is important to accurate prediction. 11.0.4.1 Lack of Causality Reminder As a reminder, as was discussed in the inferential analysis, just because one variable may predict another, it does not mean that one causes the other. In predictive analysis, you are taking advantage of the relationship between two variables, using one variable (or one set of variables) to predict a second variable. Just because one variable accurately predicts another variable does not mean that they are causally related. 11.0.5 Model Selection Additionally, there are many ways to generate prediction models. Each model was developed for a different and specific purpose. We’ll discuss a few types of predictive models here, with a focus on using linear regression. However, regardless of which model you choose to use for prediction, it’s best to keep in mind that, in general, the more data you have and the simpler your model is, the best chance you have at accurately predicting future outcomes: More data - The more observations you have and the more variables you have to choose from to include in your model, the more likely you are to generate an accurate predictive model. Note, however, large datasets with lots of missing data or data that have been incorrectly entered are not better than small, complete, and accurate datasets. Having a trustworthy dataset to build your model is critical. Simple Models - If you can accurately predict an individual’s height by only considering that person’s parents height, then go for it. There’s no need to include other variables if a single variable generates accurate predictions. A simple model that predicts accurately (regardless of the dataset in which you’re predicting) is better than a complicated model. 11.0.5.1 Regression vs. Classification Before we jump into discussing the various models you can use for predictive analysis, it’s important to first note the difference between regression and classification. Regression is used when you’re trying to predict a continuous variable. For example if you’re trying to predict an individual’s age, you would use regression. On the other hand classification is used for categorical variables, as it predicts which group an individual belongs to. An example of a classification would be predicting someone’s education level, as there are only a limited number of groups into which one would be. Regression vs. Classification With regards to machine learning, certain methods can be used for both regression and classification, while others are designed exclusively for one or the other. In this lesson we’ll discuss one regression model and one classification model. However, there are literally hundreds of models available for predictive modeling. Thus, it’s important to keep in mind that we’re really just scratching the surface here. 11.0.5.2 Linear Regression Just like in the previous lesson in inferential analysis, linear regression is an incredibly powerful method in machine learning! The concept here is the same as it was in the last lesson: we’re going to capitalize on the linear relationship between variables. However, instead of using linear regression to estimate something about a larger population, we’re going to use linear regression for prediction of a continuous variable. linear regression To better understand this, let’s use a conceptual example. Consider trying to predict a child’s age from their height. You’d likely expect that a taller child was older. So, let’s imagine that we’re looking here at the training data. We see the expected relationship between height and age in this scatterplot. training data for age and height example Using the training data, linear regression is then carried out to model the relationship. linear regression models the relationship between a child’s height and their age in the training data Now that we have our model, we no longer care about the individual data points in the training data. We’ll simply use the linear regression model to make our predictions. Our linear regression model will be used for prediction Then, in the future when we know a child’s height, we can return to our linear regression, supply it with the new child’s height and it will return the child’s age using the model we’ve built. predicting age from height using linear regression Conceptually, this is what will happen whenever we use linear regression for machine learning. However, it will be carried out mathematically, rather than graphically. This means you won’t have to look on the graph to see your predictions. You’ll just have to run a few lines of code that will carry out the necessary calculations to generate predictions. Additionally, here we’re using a single variable (height) to model age. Clearly, there are other variables (such as a child’s sex) that could affect this prediction. Often, regression models will include multiple predictor variables that will improve prediction accuracy of the outcome variable. 11.0.5.3 Classification and Regression Trees (CART) Alternatively, when trying to predict a categorical variable, you’ll want to look at classification methods, rather than regression (which is for continuous variables). In these cases you may consider using a classification and regression tree (CART) for prediction. While not the only classification method for machine learning, CARTs are a basic and commonly-used approach to prediction for categorical variables. Conceptually, when using a CART for prediction, a decision tree is generated from the training data. A decision tree branches the data based on variables within the data. For example, if we were trying to predict an individual’s education level, we would likely use a dataset with information about many different people’s income level, job title, and the number of children they have. These variable would then be used to generate the tree. For example, maybe the first branch would separate individuals who make less than 40,000 dollars a year. All of those in the training data who made less than 40K would go down the left-hand branch, while everyone else would go down the right-hand branch. Start to generate branches for your decision tree using the data to make decisions At each level, the data will continue to be split, using the information in the training data. Branches continue to be generated from the training data Finally, a full decision tree will be constructed, such that there will be a label for the variable we’re trying to predict at the end of each branch. labels are assigned at the end of each tree This CART will then be used for prediction in future samples. Thus, if you follow the path along the decision tree, for this example CART, an individual who made more than $40,000 a year, was in a manual labor profession, and had children, this CART would predict that that individual’s education level were “High School.” Predictions are then made following the decisions on the tree Again, this is conceptually and graphically how a CART works; however, when generating a CART yourself, it again only takes a few lines of code to generate the model and carry out the necessary math. 11.0.6 Model Accuracy A common saying is that prediction is hard, especially about the future. This is true in predictive analysis. Thus, it’s important to always carefully evaluate the accuracy of your model and to never overstate how well you are able to make predictions. Generally, if your predictions are correct, you’re doing well! If your predictions are wrong, you’re not doing ass well. But, how do we define “well”? 11.0.6.1 Error Rates To assess whether or not our predictive models are doing well, we calculate error rates. The two most common ways to assess how well our predictive models are doing are: RMSE (Root-mean-square Error) Accuracy We’ll note here that in order to assess error, you have to know the truth (the actual value) in addition to the predicted value. Thus, RMSE and Accuracy are assessed in the training and tuning data, where you know the actual value as well as the predicted value. 11.0.6.1.1 RMSE The root-mean-square error (RMSE) is a measure used to assess prediction error for continuous variables. Generally, we want to minimize error in prediction. Thus, a small RMSE is better than a large RMSE. RMSE Mathematically speaking, the RMSE is the square root of the variance. From earlier lessons, we know that variance has something to do with how confident we are in our estimate. Since we’re trying to determine how close our predictions are to the actual value, this seems like a good place to start. When we look at the equation, we can see that the difference between the predicted and actual values is calculated (Predicted - Actual) and that this value is then squared (Predicted - Actual)^2. These differences squared are then added for every individual in your dataset (that’s what the sigma, or big E says). This value (the sum of all the errors squared) is then divided by the number of individuals in your dataset (N). This square root of this value is then taken. This is how RMSE is calculated. We went through that description because we want to point out that when differences are squared (Predicted - Actual)^2, outliers, or samples whose prediction was far off from their actual value are going to increase the RMSE a lot. Thus, a few outliers can lead to really high RMSE values, even if all the other predictions were pretty good. This means it’s important to check to see if a few outliers (meaning a few bad predictions) are leading to a high RMSE value. 11.0.6.1.2 Accuracy Alternatively, to assess error in the prediction of categorical variables, accuracy is frequently used. Accuracy looks to determine the number of predictions that match their actual values. Accuracy The closer this value is to 100%, the better your predictive model was. The closer to 0%, the worse your model’s predictions are. Accuracy is a helpful way to assess error in categorical variables, but it can be used for numeric values too. However, it will only account a prediction “correct” if it matches exactly. In the case of age, if a sample’s age is 10 and the model predicts it to be 10, the model will say it’s been predicted correctly. However, if a sample’s age is 10 and it is predicted to be 9, it will be counted as incorrect, even though it was close. A prediction off by a year will be marked just as incorrect as a sample predicted off by 50 years. Due to this, RMSE is often opted for instead of accuracy for continuous variables. 11.0.7 Machine Learning Examples To better understand all of the concepts we’ve just discussed, we’ll walk through two examples, one for prediction of a continuous variable using linear regression and a second for prediction of a categorical value using a CART. 11.0.7.1 The caret package There is an incredibly helpful package available in R thanks to the work of Max Kuhn. As mentioned above, there are hundreds of different machine learning algorithms. Max’s R package caret has compiled all of them into a single framework, allowing you to use many different machine learning models via a single package. Additionally, he has written a very helpful book to accompany the package. We’ll be using this package throughout these examples. 11.0.7.2 Continuous Variable Prediction: Linear Regression For this example, we’ll keep it simple and use a dataset you’ve seen before: the iris dataset. This way you can focus on the syntax used in the caret package and the steps of predictive analysis. In this example, we’ll attempt to use the data in the iris dataset to predict Sepal.Length 11.0.7.2.1 Data Splitting As mentioned above, one of the first steps is often to take your dataset and split it into a training set and a tuning set. To do this, we’ll load the caret package and use the createDataPartition() function to split the dataset. ## install and load packages install.packages(&quot;caret&quot;) library(caret) library(dplyr) ## get Index for training set set.seed(123) trainIndex &lt;- createDataPartition(iris$Species, p = .7, list = FALSE, times = 1) ## split into training and tuning set iris_train &lt;- iris %&gt;% slice(trainIndex) iris_tune &lt;- iris %&gt;% slice(-trainIndex) ## take a a look str(iris_train) str(iris_tune) After running this code , if we take a look at the training and tuning datasets, we can see that 70% of our observations are in the training dataset and the other 30% are in the tuning dataset, as we specified. structure of training and tuning data 11.0.7.2.2 Variable Selection What if we first try to predict Sepal.Length in our training data from Sepal.Width. To do that, we provide the train function with the model and specify that the dataset we’ll be using is the iris dataset. Additionally, we let the train function know that we want to run linear regression (lm) and that we want to assess our model’s accuracy using the RMSE metric. ## train regression model set.seed(123) fit.lm &lt;- train(Sepal.Length ~ Sepal.Width, data = iris, method = &quot;lm&quot;, metric = &quot;RMSE&quot;) After training the model, we take a look at our RMSE, and see that it is 0.82 for this dataset. ## look at RMSE fit.lm$results RMSE Using this model, we would then generate predictions of Sepal.Length in the tuning dataset using the predict() function. Since we know the actual Sepal.Length in the tuning set, these predictions can then be visualized using a scatterplot. ## make predictions in tuning data set predictions &lt;- predict(fit.lm, iris_tune) ## visualize results iris_tune %&gt;% mutate(predictions = predictions) %&gt;% ggplot() + geom_point(aes(Sepal.Length,predictions)) Scatterplot shows Sepal.Length is not predicted well from Sepal.Width alone Given the lack of correlation, we can see that this model does not predict sepal length in our tuning set well. In this first attempt, we specified which variable to use for prediction; however, what if we provided our regression model with all the variables in the dataset (specified by the . in the code here: ## train regression model set.seed(123) fit.lm2 &lt;- train(Sepal.Length ~ ., data=iris, method=&quot;lm&quot;, metric= &quot;RMSE&quot;) ## look at RMSE fit.lm2$results ## make predictions in tuning data set predictions2 &lt;- predict(fit.lm2, iris_tune) ## visualize results iris_tune %&gt;% mutate(predictions = predictions2) %&gt;% ggplot() + geom_point(aes(Sepal.Length,predictions2)) Scatterplot 11.0.7.2.3 Accuracy Assessment Now when we look at the results we visually see improvement in the Sepal.Length predictions within the tuning dataset, which is also reflected in the decreased RMSE (0.324). Here, by including additional variables (often referred to as features in machine learning), we see improved prediction accuracy. There are more robust ways than trying a number of different variables in your model to select which should be included in your predictive model. These will be covered in lessons in the advanced track of this Course Set. 11.0.7.2.4 Model Selection In this example (and the example below), we’ve pre-specified which model we were going to use for the example ahead of time. However, there are many different regression models from which we could have chosen and a number of parameters in each that can be tuned, each of which can improve the predictive accuracy of your model. Learning how to choose and tune the best model will be discussed in lessons in the advanced track of this Course Set; however, for now we’ll note that, as specified in the caret book, the train() function has a number of capabilities. It can: evaluate how different tuning parameters in the model affect performance choose the “optimal” model, given these parameters estimate model performance (given a training set) Here, we haven’t played around much with the tuning parameters; however, checking out the documentation on how to do this can lead to improved prediction as you generate predictive models on your own. 11.0.7.3 Categorical Variable Prediction: CART A more natural prediction model given this dataset may be to predict what Species a flower is, given its measurements. We’ll use the iris dataset to carry out this classification prediction here, using a CART. 11.0.7.3.1 Data Splitting Data splitting from above will be used here. Thus, our training set will still be iris_train and our tuning set iris_tune. 11.0.7.3.2 Variable Selection Given the relatively small nature of this dataset, we’ll build the CART using all of the data; however, further and more robust optimization of what variables are included in the model is possible within the caret package. Here we specify that we want to predict Species, that we want to use a CART to do so by setting the method to rpart, and that, since it’s a categorical variable, we’re going to use Accuracy to as our assessment metric. ## CART set.seed(7) fit.cart &lt;- train(Species~., data = iris, method = &quot;rpart&quot;, metric = &quot;Accuracy&quot;) ## look at Accuracy fit.cart$results ## make predictions in tuning data set predictions_cart &lt;- predict(fit.cart, iris_tune) table(iris_tune$Species, predictions_cart) 11.0.7.4 Accuracy Assessment table() output Here, we see that in the tuning data, the CART accurately predicted the Species of most flowers using the model generated from the training data; however, it did make two incorrect predictions (the 1s in the table). 11.0.8 Summary In this lesson we have covered the basics of what predictive analysis is, what types of prediction are commonly done, and how to carry out a predictive analysis using the caret package. Surely, this lesson has only introduced the very basics of machine learning, and there is still a lot out there to learn beyond what is in this lesson! 11.0.9 Additional Resources The caret Package book Example of machine learning using caret and the iris dataset for classification 11.0.10 Slides and Video Automated Videos Slides "],["data-analysis-workflow.html", "Chapter 12 Data Analysis Workflow", " Chapter 12 Data Analysis Workflow In previous lessons, we went over the basics of data analysis. We went from how to ask data science questions and finding data to inferential and predictive data analysis. Since as a data scientist you may end up working on different projects at the same time. To prevent forgetting important steps that you learned in this course it’s crucial that you follow all the steps. In this lesson, we’re going to talk about a workflow for your data analysis projects. 12.0.1 What are the steps? To begin with, these are the main steps you have to follow in your data analysis question: Define the question Define the ideal dataset Determine what data you can access and obtain the data Clean the data Exploratory data analysis Statistical analysis Interpret results Challenge results Synthesize/write up results Create reproducible code 12.0.2 An example Let’s start with a hypothetical example and go through each step in our example. Imagine we’re interested to automatically detect emails that are SPAM from the ones that are not. So our general question is “can I automatically detect emails that are SPAM from the ones that are not?” Detecting SPAMs 12.0.2.1 Define the question However, this question is not written entirely in data science terms. We have to make sure our question can be measured and quantified with data. We have to make our question more concrete. So a better way to ask the question is this: “Can I use quantitative characteristics of the emails to classify them as SPAM?” 12.0.2.2 Define the ideal dataset The second step in data analysis is to imagine an ideal dataset for our analysis. You don’t have to be practical in your thinking. Just imagine what type of data would be best for your analysis. In an ideal world, you would want a dataset of all emails received through major email providers such as Gmail or Yahoo and whether the email was flagged as SPAM or not. 12.0.2.3 Determine what data you can access and obtain the data You may be lucky to have such dataset somehow. However, it’s unlikely that due to privacy reasons you can access other people’s emails. Even if you can, the data will be gazillions of bytes and it won’t be practical to analyze such a large dataset. So our best bet is to see if there’s any dataset online. One of the best datasets for analyzing SPAM data is the spam data in the kernlab package in R. The spam dataset is collected at Hewlett-Packard Labs and classifies 4601 e-mails as spam or non-spam. Additionally, there are 57 variables indicating the frequency of certain words and characters in the e-mail. Let’s install the package first. spam dataset from the kernlab package library(kernlab) data(spam) 12.0.2.4 Clean the data In most cases, your data doesn’t come clean. In fact, it may come from different sources with different standards. Therefore, you should first tidy up the data. Lucky for us, the spam dataset in the kernlab package is already tidy, so we can skip this step. However, if we’re doing predictive analysis, it’s better to have a training and a test set. The code below creates the train and test sets. Cleaning the data 12.0.2.5 Exploratory data analysis We learned about the following steps for doing exploratory data analysis: Look at summaries of the data Check for missing data Create exploratory plots Perform exploratory analyses First, we look at column names. Looking at column names And the first few rows of our training data. Looking at the first few rows of the data Let’s see how many of the emails are flagged as SPAM and how many are not. 906 emails in the training set are flagged as SPAM We can also plot the average length of capital letters in the text of the email for SPAM and non-SPAM emails. The variable in the data that measures the average length of capital letters in the text is called capitalAve. Plotting the average length of capital letters To better distinguish the difference in capitalAve for SPAM and non-SPAM emails, we can use the log scale. We can convert the variable to log. Be careful that if you have zeros in your data (which you may have), by transforming the variable into log you will run into trouble (log of zero is infinity). To avoid this, we can add 1 to the variable. Plotting the log of average length of capital letters We can see if there is any relationship between some of the predictors such as free, original, and receive. Relationship between some of the predictors 12.0.2.6 Statistical analysis The type of analysis that we need is predictive analysis since, at the end of the day, our algorithm should predict whether an email is SPAM or not. Note that: Exact methods depend on the question of interest Transformations/processing should be accounted for when necessary Measures of uncertainty should be reported We can use the following code to perform our prediction analysis using the training set. Note that the function cv.glm() calculates the estimated K-fold cross-validation prediction error for generalized linear models (glms). The code chunk below finds the variable (among all of our variables) that has the lowest prediction error in finding the probability of being SPAM. Prediction analysis We can then use the test set to get a measure of uncertainty (or accuracy) of the model. For each observation in the test set, we predict whether the observation is a SPAM or not. Note that we already know whether the observation is a SPAM or not but we want to test our model’s ability. Once we find the predicted values, we can use them along with the actual values (whether the observations are indeed SPAM or not) and create an error matrix. The error matrix shows how many of the SPAM emails we thought were SPAM and how many we didn’t. The same for non-SPAM emails. The line table(predictedSpam,testSpam$type) shows that there are 61 non-SPAM emails that our model predicted as SPAM and 458 SPAM emails that our model predicted as non-SPAM. The rest of the observations were predicted correctly. The last line of the code calculates the prediction error. Prediction error 12.0.2.7 Interpret results Once you have done the preliminary analysis, you should interpret the results so others know what conclusions can be made from your analysis. You should be careful not to confuse the following words: Describes (only if you observe a phenomenon without doing any inferential or predictive analysis) Correlates with/associated with (only if you look at the association between variables without any causal interpretation) Leads to/causes (only if you have performed causal inference analysis) Predicts (only if you have performed predictive analysis) Make sure you give enough explanation to your analysis. Give an explanation as to what your numbers are telling (and not telling) If you do regression analysis, interpret the coefficients Interpret measures of uncertainty In our example, here are some of the interpretations we can give. The fraction of characters that are dollar signs can be used to predict if an email is Spam Anything with more than 6.6% dollar signs is classified as Spam More dollar signs always means more Spam under our prediction Our test set error rate was 22.4% 12.0.2.8 Challenge results A good analyst is a good critique of his/her work since the analyst knows the data and the methods best. Your self-critique and self-challenge should start from the start: the question. Challenge Question: Start from your question. Is your question asked properly? Data source: Challenge your data and make sure your data is good enough for your question. Processing: Check whether the data cleaning and processing are done properly. Analysis: Challenge your methods; is it the best method you could use given your question and your data. Conclusions: Check whether your conclusions are drawn properly. Measures of uncertainty: There are various ways to measure the uncertainty of your model. Check whether you have used the best measure. Choices of terms to include in models: You always include specific variables in your model. Make sure you have included the ones that make sense regardless of whether they make your results look good. Finally, think of potential alternative analyses: Recognize that there might be alternative approaches to your question such as the use of data, method, etc. Recognizing them will show some honesty on your part and will pave the way for future analyses. 12.0.2.9 Synthesize/write-up results Once you’re done with the analysis and checking the credibility of your analysis, start writing up your results. These are some of the important steps: Lead with the question Summarize the analyses into the story Don’t include every analysis, include it If it is needed for the story If it is needed to address a challenge Order analyses according to the story, rather than chronologically Include “pretty” figures that contribute to the story In our example we should: Lead with the question Can you use quantitative characteristics of the emails to classify them as SPAM/HAM? Describe the approach The source of our SPAM data and how we created training/test sets Explored relationships Choose logistic model on the training set by cross-validation Applied to test, 78% test set accuracy Interpret results Number of dollar signs seems reasonable, e.g. “Make CASH from home \\$ \\$ \\$ \\$!” Challenge results 78% isn’t that great I could use more variables Why logistic regression? 12.0.2.10 Create reproducible code As you learned in previous lessons, make sure you document every step. This is important for two reasons: for you in future and for others to redo your analysis. Using Rmarkdown is a good way to accompany your analysis with good documentation. So it’s important that: Files are properly named. There is some explanation of the data. Each code file has some description as to what it does. Wherever you should add comments for important code chunks within your code files. 12.0.3 Slides and Video Automated Videos Slides "],["data-analysis-pipelines.html", "Chapter 13 Data Analysis Pipelines", " Chapter 13 Data Analysis Pipelines In data science jobs in industry, often you’re analyzing a dataset over time. What this means is that the dataset’s structure doesn’t change – the variables you’re collecting remain the same – but the observations do. Maybe you’re collecting sales figures for a company, and each month you’re reanalyzing the dataset to see if trends have changed or if your predictions need to be altered. Or, possibly, you’re analyzing user data where the demographics and number of users of your product change over time. In these cases, where the variables don’t change but the observations do, pipelines can be incredibly helpful. 13.0.1 Pipelines Pipelines are a series of steps that can be applied to a dataset. It’s helpful to generate a pipeline whenever you come across a dataset that you expect to encounter again. A pipeline could read in a dataset, process that dataset to get it into a tidy format, generate figures to summarize the data, and generate predictions. This pipeline could then be set to run any time new data are generated or on a time table, say every month. In this sense, data science pipelines should be automated. In other words, after you build the pipeline, you shouldn’t have to do any additional work each time you run the pipelines. Additionally, pipelines have the additional benefit that they are reproducible. This is helpful to you, since you’ll be able to reproduce your results at any point in time. But, this is also helpful because they can be used by anyone who has access to the data. The pipeline will work if you leave the company and someone takes over for you or if you’re sick and need to take a day off. 13.0.2 Considerations When generating a pipeline, there are a number of considerations to take into account before and while you’re developing the pipelines. Here, we’ll discuss when to consider a pipeline, what to consider while you’re generating the pipeline, and how to track changes that are made to the pipeline. 13.0.2.1 When to make a pipeline Pipelines are helpful when you have a question that you will answer more than once. For example, if you have data that will be updated over time, a pipeline may be helpful. If, however, you’re doing a quick analysis on a single set of data to answer a question that you will not likely have to answer again in the future, then just do the analysis and get your answer. This is not the time to generate a pipeline. But, if you’ll likely run the analysis again in the future once more data has been collected, once you have updated numbers, or once you have data from more people, then generate a pipeline. Generally, pipelines are helpful when there is a single question you will have to answer more than once. 13.0.2.2 Including appropriate checks Once you’ve decided that a pipeline will be helpful, the goal is that you’ll generate a pipeline that, once you click a button, the entire analysis process will run and you’ll get your answer. In order for this to happen, your pipeline will require checks before it gets started. These checks should look to see: are the data in the **expected format*? - Are we expecting tabular data with 10 columns? If so, does in the input data look as expected? are the necessary variables included in the dataset? - If we’re going to be using the variable sex, is that in there? If not, is there a column called Sex or gender, and is that the column we want to use? are the observations for the variables coded as we expect? - if we’re expecting females to be coded as female, is that what’s in the data? Or are they coded as F, or even 0?) Checks to ensure that the input data is the right type of data for the pipeline to run are incredibly important. These are just a few checks you may want to run on your data. You may also want to check to ensure that the data are from the dates expected or check to be sure that the data aren’t unexpectedly truncated. Considering what checks you’ll need to make on your data is an important first step to generating a pipeline. 13.0.2.3 Avoiding Hard-Coding In developing pipelines, you’ll generally want to avoid hard coding whenever possible. Hard coding in a pipeline refers to setting a parameter in a pipeline that can not be altered without going in and editing the code. For example, if while generating a pipeline for the first time you’re working with a dataset that has 100 observations. What if you specify that the expected dataset should have 100 rows and write a check for that. Well, the next time you go to run the report and you get an error because your new dataset has 150 observations. By hard-coding in the number “100” as a parameter in your pipeline, you would forced your pipeline to fail unnecessarily. Avoiding hard-coding whenever possible is a good rule of thumb as you generate pipelines. This means that you will just specify the input to your pipeline (which could include multiple parameters!) and your pipeline will run using the data you’ve input and the code in your pipeline to generate the results. You won’t have to go in and change any numbers that you’ve hard-coded in your pipeline. 13.0.2.4 Scalability Additionally, your pipeline should be scalable. A scalable pipeline is one that will function on a small dataset and will continue to function as the dataset gets larger. It’s best to ensure that your pipeline has been generated so that it will run on your dataset as well as larger datasets that may be generated in the future. 13.0.2.5 Versioning Finally, pipelines should be versioned. If the format of the dataset changes in the future, you’ll want to be able to update your pipeline, but you don’t want to loose the original pipeline, since that’s what works on the older datasets. Or, if an API changes (which they do!), you’ll need to update your pipeline to account for these changes. Again, you don’t want to lose the older version. Versioning here refers to the process of tracking changes in your pipeline. Every time you’ve pushed to GitHub, you’ve been using a type of versioning. GitHub tracks commits, which keeps track of every version of your work. However, in pipeline development, it’s best to explicitly track the versions of your pipelines. In software each version of a piece of software is often tracked by a set of three numbers, each of which is separated by a decimal place. A general scheme used in software versioning is to version a piece of software as major.minor.patch. In this scheme, a large release with breaking changes (changes that could cause previous versions of the software to be incompatible with the current version) will increase the first number in the sequence (major). The second value (minor) will be increased when a version of the software is released that includes new features that are non-breaking. Versions where the the third value (patch) in the sequence are increased include non-breaking changes that tend to be minimal fixes to the software. major.minor.patch In this scheme, 0.9.0 would represent software that is in development (referred to as the beta version). Incremental improvements on the beta version would be versions 0.9.1 and 0.9.2, etc. The major number remains a zero until the first major release. Upon release, the first version of the software would be 1.0.0. Small non-breaking changes to this version would increase the patch number, making versions like 1.0.1 and 1.0.2. When new features are added to this piece of software that are non-breaking, the second minor number will be increased, and version 1.1.0 would be released. The next time a non-breaking feature was added, version 1.2.0 would be released. Finally, when a big breaking change were made in the future to improve the software overall, a new version would be released. This would be version 2.0.0. versioning scheme This scheme from software development can be very helpful in pipeline development. While in development, a pipeline should be versioned such that the first version of the pipeline is 0.9.0. As improvements are made, the pipeline can increase its versioned as discussed above (0.9.1, 0.9.2, etc.). When the pipeline is ready to be officially deployed, its version will be 1.0.0. Then, as changes are made and the pipeline improved, versioning will allow for these changes to be tracked. 13.0.3 Parameterized Reports In R, one of the simplest ways to generate a pipeline is to write a parameterized report. Parameterized reports can be generated by starting with an R Markdown document (with which you’re already familiar!) and adding additional information to the YAML at the top of the document. Parameters can be defined at the top of the report using the params field. For example, if you wanted to specify the file you were using in your report, you could define a parameter filename within the params field. Then, when rendering the report, you would specify which file to use to generate the report, as you see here: --- title: My Document params: filename: filename_filedate.csv --- This file could then be rendered as an argument within the rmarkdown::render() function. The filename argument could be updated to use whichever file you want to use to generate the report: rmarkdown::render(&quot;MyDocument.Rmd&quot;, params = list( filename = &quot;filename_filedate.csv&quot;) ) Note that for any parameter not specified within rmarkdown::render(), the default value within the R Markdown document will be used. 13.0.4 Pipeline Example Survey Results To really understand how a pipeline would be generated, let’s work with an example set of data. In a previous lesson, we talked about how you can read in a Google Sheet using the googlesheets package to read in the most up-to-date version of the spreadsheet. In this previous lesson we talked about surveying your friends about how many hours they spend each day working, sleeping, having fun, eating, socializing, and other. We’ll use this example again here to generate a parameterized report that will analyze these data as they’re updated. 13.0.4.1 The Data The data we’ll use for this example can be viewed here. For our purposes, we’re going to say that the first Sheet (Sheet1) is the data that were collected after the first week the survey ran, while Sheet 2 contains all data collected in the first two weeks. Google Sheets data Imagine that a week after you’ve sent the survey out to your friends, you’re really curious about the results. Thus, you write a parameterized report to visualize the results! You do this because you want to be able to analyze the data at the click of a button and because you know you’ll generate this report again in the future once more data have been collected. 13.0.4.2 Following Along If you’re interested in running this pipeline and following along on your own, the code used in this lesson can be found here. You can make a copy of this space in RStudio Cloud and generate your own reports. The URL you’ll want to include when prompted is the following URL: https://docs.google.com/spreadsheets/d/1MpGE4YHB14qBgrg3lqa1eq_Mb7L8TMOwTZtVzqQ0mmA. To get started writing the parameterized report, you’ll want to set up your report in the R Markdown format and read your data in. 13.0.4.3 The Setup To get started, you’ll want to open up a new R Markdown document. New R Markdown Document In this document you’ll want to specify two parameters using params: YAML of R Markdown The file_url parameter says “ask”. This means that when we go to knit the report, we want to RStudio to ask at that point what URL to use for the Google Sheet. The second parameter, worksheet, specifies that we want the default to be the first worksheet, but we give ourselves the flexibility to specify a different worksheet, if we so choose. After setting up the YAML, we’ll want to install and load any packages that we’ll use in the setup code chunk: setup code chunk Here, we’re installing four packages. In this code we state that if the packages haven’t already been installed, do so. Then, all packages are loaded into our RStudio Cloud session. 13.0.4.4 Reading The Data In After setting up the RStudio Cloud environment, the data are read in in the code chunk data. data code chunk Using gs_url() and gs_read() from the googlesheets package we’re able to read in the Google Sheet and specify which worksheet we want to include for analysis. Notice the use of the params object. When this file is knit, it will take the params specified in the YAML as the input for compiling the report. We’ll see exactly how to do this later in this lesson. Note that including a parameter always begins with the object params. This is followed by a dollar sign ($). Finally, you use the name specified in the YAML to specify which parameter you want to include in your code. Specifically here, we first specify that we want to register the Google Sheet using params$file_url and then specify which worksheet to read in by specifying params$worksheet. 13.0.4.5 Checks Before actually writing the code to analyze the data, you want to be sure your report runs the appropriate checks on the data itself. As discussed earlier in this lesson, these checks should ensure that the data are in the format the rest of the code is expecting. For example, if you accidentally specified the wrong file on which to run the report, you’d want your report to send you a warning letting you know that the data were not what the report was expecting. Checks are incredibly important parts of a data science pipeline and should not be overlooked. We will only include a single check here in this example; however, for most pipelines, you’ll certainly want to write a few more checks to ensure that your analysis will run properly. Always try to anticipate possible mistakes that users could make in generating this report. Then, write checks to test for each possible scenario. Remember, others should be able to generate this report in your absence. Here, we would add this code to a code chunk called checks. checks code chunk In the code in this chunk (above), we’re checking to see if the expected column names are what are in the Google Sheet URL that was provided. You’ll see we specify what the expected column names are by creating the columns object. Then, the check looks to see how many columns in the Google Sheet are the same as the columns specified in the columns object. If they are not all equal, then the report will not compile and an error will be thrown. Note that there are three types of feedback your code can give in R: message - prints a message but not stop execution warning - prints a warning message but not stop execution stop - stops execution of the current expression and executes an error action In this code, we’re providing a stop() message because we don’t want the report to carry out the analysis unless the data are in the expected format. However, in other cases, you may just want to print a message in the output or provide the user with a warning but not stop the report from compiling. In these cases you would use message() and warning(), respectively. Finally, note that in this code chunk we’ve included echo = FALSE. This evaluates the code so the check is carried out, but doesn’t print it in the output report. Once you’ve written enough checks to ensure that your report will only compile when you have the right data in the right format, you’re ready to begin the analysis. 13.0.4.6 The Analysis To analyze the data, maybe you’re just interested in summarizing the mean number of hours your friends spend doing each activity. To do this, you would first clean the data and then generate a plot. 13.0.4.6.1 Data Cleaning Before you can generate this plot, you will want to check and clean your data. For this example here, what if people said they spent more than 24 hours doing things in a day? Well, we know there are only 24 hours in a day, so you’d likely want to exclude these individuals from the analysis, as you won’t know for sure where the error in their data entry happened. You would write a few lines of code to do this using your data wrangling skills. clean code chunk Here, we have specified that we don’t want this code chunk included in the final report, using include = FALSE in the code chunk. We have also included comments so that anyone looking at the raw code would know what the code does. But, the final knit report will not include such details. You’d also want to be sure that your report provides a description of how many individuals’ data are included in your analysis and how many samples have been removed (an example of a descriptive analysis!). Below the code chunk, we include a brief description of the data by specifying how many samples are included in the analysis and how many have been removed due to issues with their data using Markdown syntax. None of the numbers here have been hard-coded. They will all automatically populate with the appropriate number when the report is knit. 13.0.4.6.2 Data Analysis Once you’re confident that your data are ready to be used for analysis, you’d write the code to generate the plot. Here, we’re not focusing on the plot, so we use very basic code, but if this were a report you were sending around to your boss or team, you’d likely want to improve the labels, increase the font size, and maybe change the theme. Nevertheless, with this, you’re ready to knit your parameterized report. analyze code chunk 13.0.4.7 Knitting To generate your knit report, you’ll click on the arrow to the right of “Knit” and select “Knit with Parameters…” from the drop-down menu. Knit with Parameters… If prompted, say yes to installing necessary packages. Then, a window will appear where the file_url box will say “ask”, indicating that you’ll have to specify the URL here. The default worksheet is 1. You can leave this as is, or change it to specify a different worksheet. Knit with Parameters pop-up window After adding the URL for the appropriate Google Sheet, you’ll want to click “Knit” After adding URL, click Knit Your report will appear! Here, we see a summary of the data after the first week of collecting information from your friends. Report with survey data We see that most of your friends’ time is spent working and sleeping, which makes enough sense! However, the whole point of this report is that you can run it again with ease for an updated dataset – as that’s the whole point of a pipeline. The second sheet in this Google Sheet represents your updated data. Thus, we’ll click “Knit with Parameters” again and specify the appropriate URL (the same as earlier), but change worksheet to ‘2’. We’ll again Knit our report. Change parameter and re-Knit Just like that, we see the data summarized across more of our friends! People are still working and sleeping the most; however, we see a lot more variability in this updated dataset! Report with updated survey data 13.0.5 Summary This lesson has covered what a data science pipeline is, when you’d want to generate one, considerations to make when writing a pipeline, and a simple example of how to generate a data science pipeline using parameterized reports in R Markdown. Whenever you’re going to be working with a dataset that updates or a number of different datasets that are similar, start by writing a robust data science pipeline. 13.0.6 Additional Resources Building a Data Pipeline from Scratch, by Alan Marazzi Parameterized R Markdown reports with RStudio Connect RMarkdown Parameterized Reports Chapter 15: Parameterized Reports in R Markdown: The Definitive Guide, by Yihui Xie, J.J. Allaire, and Garrett Grolemund 13.0.7 Slides and Video Automated Videos Slides "],["final-project.html", "Chapter 14 Final Project", " Chapter 14 Final Project Throughout this course set, we’ve had a number of projects. This is the last project of the course set that we call the Final project. All of these will be included as Exercises. This means they will not be required to pass the course and receive your certificate; however, completing them will really help improve your understanding of the material covered and to ensure that you’ve begun to master the skills needed to be a data scientist. In each project, we’ll aim to get you started and to ask questions that will help guide you through the project. But, we’ll intentionally leave pieces out where you have to figure out what needs to be done. This project will require you to: import data from the American Time Use Survey clean and tidy the data visualize the data carry out an exploratory data analysis carry out an inferential analysis You can access the project by going to the exercise accompanying this lesson. 14.0.1 Slides Slides "],["final-project-exercise.html", "Chapter 15 Final Project Exercise", " Chapter 15 Final Project Exercise * If you would like to know the answers to the questions in this exercise, then you can take this course in Leanpub. This exercise has been generated to practice everything you have learned in this course set. 15.0.1 GitHub Setup To get started, you’ll want to go to GitHub and start a new repository: Call this repository final_project. Add a short description Check the box to “Initialize this repository with a README. Click Create Repository Once the repository has been created, Click on Clone or download and copy the “Clone with HTTPS” link provided. You’ll use this to clone your repo in RStudio Cloud. Note: If you’re stuck on this, these steps were covered in detail in an earlier course: Version Control. Refer to the materials in this course if you’re stuck on this part of the project. 15.0.2 RStudio Cloud Setup Go to the Cloud-based Data Science Space on RStudio Cloud Click on the “Projects” tab at the top of the workspace Make a copy of the project: final_project In this project you should see a final_project.Rmd file. You’ll use this to get started working on your project! Note: If you try to Knit this document at this time, you will get an error because there is code in this document that has to be edited (by you!) before it will be able to successfully knit! To start using version control, you’ll want to clone the GitHub repository you just created into this workspace. To do so, go to the Terminal and clone your project into this workspace. A new directory with the name of your GitHub repository should now be viewable in the Files tab of RStudio Cloud. You are now set up to track your project with git. 15.0.3 Data Science Project Setup As discussed previously, you’ll want all your data science projects to be organized from the very beginning. Let’s do that now! First, use cd to get yourself into the directory of your GitHub Project. Once in the correct directory, use mkdir in the terminal to create folders with the following structure: - data/ - raw_data/ - tidy_data/ - code/ - raw_code/ - final_code/ - figures/ - exploratory_figures/ - explanatory_figures/ - products/ - writing/ Now that your directories are set up you’ll want to use the Terminal (or ‘More’ drop-down menu in the Files tab) to move (mv) the final_project.Rmd file into code/raw_code in your version controlled repository**. This ensures that your code file and raw data are in the correct directory. Once the .Rmd document is in the correct folder, you’ll want to change the author of this document to your name at the top of the .Rmd document (in the YAML). Save this change before moving to the next step. Note: If you’re stuck on this, these steps were covered in detail in an earlier course: Organizing Data Science Projects. Refer to the materials in this course if you’re stuck on this part of the project. 15.0.4 Pushing to GitHub You’ll want to save changes to your project regularly by pushing them to GitHub. Now that you’ve got your file structure set up and have added a code file (.Rmd), it’s a good time to stage, commit, and push these changes to GitHub. Do so now, and then take a long on GitHub to see the changes on their website! Note: If you’re stuck on this, these steps were covered in detail in an earlier course: Version Control. Refer to the materials in this course if you’re stuck on this part of the project. 15.0.5 The Data The American Time Use Survey (ATUS) is a time-use survey of Americans, which is sponsored by the Bureau of Labor Statistics (BLS) and conducted by the the U.S. Census Bureau. Respondents of the survey are asked to keep a diary for one day carefully recording the amount of time they spend on various activities including working, leisure, childcare, and household activities. The survey has been conducted every year since 2003. Included in the data are a number of demographic variables (such as respondents’ age, sex, race, marital status, and education) as well as detailed income and employment information for each respondent. While there are some slight changes to the survey each year, the main questions asked stay the same. You can find the data dictionaries, which provide information about the variables included for each year’s survey at https://www.bls.gov/tus/dictionaries.htm. 15.0.6 Accessing the Data There are multiple ways to access the ATUS data; however, for this project, you’ll get the raw data directly from the source. The data for each year can be found at https://www.bls.gov/tus/#data. Once there, there is an option of downloading a multi-year file, which includes data for all of the years the survey has been conducted, but for the purposes of this project, let’s just look at the data for 2016. Under Data Files, click on American Time Use Survey--2016 Microdata files. American Time Use Survey You will be brought to a new screen. Scroll down to the section 2016 Basic ATUS Data Files. Under this section, you’ll want to click to download the following two files: ATUS 2016 Activity summary file (zip) and ATUS-CPS 2016 file (zip). Download Data Files ATUS 2016 Activity summary file (zip) contains information about the total time each ATUS respondent spent doing each activity listed in the survey. The activity data includes information such as activity codes, activity start and stop times, and locations. ATUS-CPS 2016 file (zip) contains information about each household member of all individuals selected to participate in the ATUS. Once they’ve been downloaded, you’ll need to unzip the files. Once unzipped, you will see the dataset in a number of different file formats including .sas, .sps, and .dat files. We’ll be working with the .dat files. 15.0.7 Loading the Data into R To use the data in RStudio Cloud, you’ll have to upload the two .dat files (atuscps_2016.dat and atussum_2016.dat) into RStudio Cloud. Make sure the data are stored in the raw_data folder. Once uploaded into the correct folder, you’ll be able to load the data in, You’ll do this by first running the setup chunk in the final_project.Rmd file and then by running the code in the atus-data code chunk. This will create an object called atus.all. How many observations are in the dataset? 15.0.8 Analyze the Data Now that we’ve got the data in RStudio Cloud, let’s get to work! 15.0.8.1 Exploratory Data Analysis Once the data have been read in, explore the data, adding your initial exploratory code to the initial-exploration code chunk in final_project.Rmd. Then, answer the following questions: You can find data dictionaries (also called codebooks) at https://www.bls.gov/tus/atuscpscodebk16.pdf for the CPS data and at https://www.bls.gov/tus/atusintcodebk16.pdf for the rest of the variables. Using this data dictionary is very important as a lot of the information about the variables in the data and how they are coded can be found here. By looking at the variables in the data frame atus.all, we see that there are a lot of variables that start with t followed by a 6-digit number. These variables capture the total number of minutes each respondent spent doing each activity. The link https://www.bls.gov/tus/lexiconwex2016.pdf lists all the activity codes. Using the information in that file, what column is associated with the activity “playing computer games”? Your answer should start with t and then a 6-digit number that combines the major category, the 2nd tier, and the 3rd tier of the activity. For instance, if major category in equal to 05, second tier is equal to 03, and third tier is equal to 01, then your answer should be t050301. In the data, the variable t010101 contains the total number of minutes each respondent spent doing activity 010101 which is “sleeping” and the variable t010102 contains the total number of minutes each respondent spent doing activity 010102, “sleeplessness.” Find the variable associated with “Socializing and communicating with others.” How much time, on average, does a person in the sample spend on “Socializing and communicating with others”? Again, The link https://www.bls.gov/tus/lexiconwex2016.pdf lists all the activity codes. Find the activity code that is associated with “Caring For &amp; Helping HH Children”. This is a level 2 activity so we need to add all the variables that start with t0301. Create a column in the data frame atus.all called CHILDCARE that is the sum of all the columns that start with t0301. Here, you’ll have to change the code chunk creating-childcare-var. Move on to the code chunk called childcare-density-plot and write code in ggplot2 to plot the density function of the variable CHILDCARE. From the data dictionary, what is the variable that shows the gender of the respondent? From the data dictionary, the variable that represents the gender of the respondents can one of take two values, 1 or 2. Which gender group does 1 represent? 15.0.8.2 Inferential Data Analysis We are going to answer whether women or men spend more time with their children. Start by grouping individuals by their gender and calculate the average time men and women spend with their children. Use the code chunk gender-analysis in the .Rmd file. Note that you should replace FUNCTION in order to calculate the average of the variable CHILDCARE. Men and women are different in the amount of time they spend with their children. Which group spends more time, men or women? Use the table() function to look at the variable TRDPFTPT which shows whether the person works full time or part time. You will notice that the variable also takes the value -1. This is probably due to non-response or other data collection reasons. Replace these values with NA in your data so they don’t affect your analysis. Use the code chunk replacing-na for doing this and add your commands there. How many NAs are in the variable now? Now, we are going to explore what factors affect time spent with children. We are going to answer questions like: Do younger parents spend more time with their children? Do richer people spend more time with their children compared to poorer people? Do married couples spend more time with their children compared to single parents? Do full-time workers spend more time with their children compared to part-time workers? Compare each of these relationships and present your results in a table or a graph. You can do this by first finding these variables from the data dictionary. Add your code in the code chunk exploratory-analysis in the .Rmd file. Make sure that in your analysis, you limit your data to those who have at least one child (18 or younger) in the household. The variable for the number of children (18 or younger) in the household is TRCHILDNUM. Use the data frame atus.all that you previously created and again limit your data to those who have at least one child (18 or younger) in the household. The variable for income is HEFAMINC. Other variables are easy to find from the data dictionary. In the exercise above, we looked at bilateral (two-way) relationships. For instance, we looked at how income and time spent with children are related. You have learned in this course, however, that other confounding variables can be a source of bias in your analysis. For instance, the effect of income on time spent with children can be biased by the number of children a person has. Maybe richer people spend less time because they have fewer children. It’s much better to look at the relationship of all relevant variables associated with time spent with children together. Run a linear regression of marital status, age, sex, number of children (18 or younger), earnings, and full-time versus part-time status. Add your code in the reg-analysis code chunk. Remember to limit the sample to those who have at least one child (18 or younger) in the household. Also make sure to to change the values of the variable TRDPFTPT that are -1 to NA. What is the coefficient on gender now? In the regression, the coefficient on the variable age means how much time spent with children changes if age increases by 1. Based on your results, what’s the difference in minutes spent with children between two people with 10 years of age difference? In the next few questions, we are going to see whether time spent on different activities varies by age. However, we will only consider activities at the major category levels. There are 18 major categories in the data including personal care, household activities, caring for &amp; helping household members, etc. Because each activity column in the data is at the 3rd tier, we will need to start by suming columns at the activity major categories levels. Save the resulting data frame as df.wide. Use code chunk activity-cats in the .Rmd file for this part. What activity group do people spent, on average, most time on? Which is the second most time consuming activity for the respondents? What is the maximum time a person in our sample spends time on “Work &amp; Work-Related Activities”? Convert the data from wide to long using the package of your choice and save the data frame as df.long. Use the code chunk wide-to-long for this purpose. Make sure your key variable is called ACTIVITY and your value variable is called MINS. Now, group the data frame you created in the previous step by activity type (ACTIVITY) and age (TEAGE). Calculate the average time for each age group and call it AVGMINS. In ggplot2, plot AVGMINS against TEAGE for each category (multiple panels). Type your code in the code chunk age-activity. Label each panel in your graph with the appropriate activity name. For which categories does the average time spent vary by age? Based on the graph, what is true about the activity category 5 (Work &amp; Work-Related Activities)? 65-year olds work the most. Young, middle-aged, and old workers spend more or less the same number of hours working. Young people work more than middle-aged people. Middle aged people work the most compared to younger and older people. Based on the graph, what is true about the activity category 12 (Socializing, Relaxing, and Leisure)? Older people spend more time socializing. Middle-aged people are the most social. Younger people socialize more than older people. Young people are the most social. 15.0.8.3 Data Visualization Finally, in this last step, we are going to create a graph that shows how different income groups spend time doing each activity. The graph is based on Henrik Lindberg’s data visualization posted here. The only difference is that we are only looking at the 18 major activity categories. Use the long data that you created in the previous section and make the graph as close as possible to the graph by Henrik Lindberg. Type your code in the code chunk activity-income. 15.0.8.4 Save the Plot Once you’ve worked through this code chunk and have a plot that looks very similar to the plot in the original publication (and that you see above), you’ll want to use ggsave() to save this plot to figures/explanatory_figures. Save this figure as “activity-income.png”. Do this using the partial code in the code chunk save-plot. 15.0.9 Add Markdown Text to .Rmd Before finalizing your project you’ll want be sure there are comments in your code chunks and text outside of your code chunks to explain what you’re doing in each code chunk. These explanations are incredibly helpful for someone who doesn’t code or someone unfamiliar to your project. Note: If you’re stuck on this, these steps were covered in detail in an earlier course: Introduction to R. Refer to the R Markdown lesson in this course if you’re stuck on this part (or the next part) of the project. 15.0.10 Knit your R Markdown Document Last but not least, you’ll want to Knit your .Rmd document into an HTML document. If you get an error, take a look at what the error says and edit your .Rmd document. Then, try to Knit again! Troubleshooting these error messages will teach you a lot about coding in R. 15.0.11 A Few Final Checks A complete project should have: Completed code chunks throughout the .Rmd document (your RMarkdown document should Knit without any error) README.md text file explaining your project Comments in your code chunks Answered all questions throughout this exercise. 15.0.12 Final push to GitHub Now that you’ve finalized your project, you’ll do one final push to GitHub. add, commit, and push your work to GitHub. Navigate to your GitHub repository, and answer the final question below! Note: If you’re stuck on this, these steps were covered in detail in an earlier course: Version Control. Refer to the materials in this course if you’re stuck on this part of the project. Congrats on finishing your Final Project!! {/exercise} "],["references.html", "Chapter 16 References", " Chapter 16 References "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     Credits Names Pedagogy Lead Content Instructor(s) FirstName LastName Lecturer(s) (include chapter name/link in parentheses if only for specific chapters) - make new line if more than one chapter involved Delivered the course in some way - video or audio Content Author(s) (include chapter name/link in parentheses if only for specific chapters) - make new line if more than one chapter involved If any other authors besides lead instructor Content Contributor(s) (include section name/link in parentheses) - make new line if more than one section involved Wrote less than a chapter Content Editor(s)/Reviewer(s) Checked your content Content Director(s) Helped guide the content direction Content Consultants (include chapter name/link in parentheses or word “General”) - make new line if more than one chapter involved Gave high level advice on content Acknowledgments Gave small assistance to content but not to the level of consulting Production Content Publisher(s) Helped with publishing platform Content Publishing Reviewer(s) Reviewed overall content and aesthetics on publishing platform Technical Course Publishing Engineer(s) Helped with the code for the technical aspects related to the specific course generation Template Publishing Engineers Candace Savonen, Carrie Wright Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Candace Savonen Package Developers (ottrpal) John Muschelli, Candace Savonen, Carrie Wright Art and Design Illustrator(s) Created graphics for the course Figure Artist(s) Created figures/plots for course Videographer(s) Filmed videos Videography Editor(s) Edited film Audiographer(s) Recorded audio Audiography Editor(s) Edited audio recordings Funding Funder(s) Institution/individual who funded course including grant number Funding Staff Staff members who help with funding   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.0.2 (2020-06-22) ## os Ubuntu 20.04.3 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2023-03-20 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] RSPM (R 4.0.3) ## bookdown 0.24 2022-02-15 [1] Github (rstudio/bookdown@88bc4ea) ## callr 3.4.4 2020-09-07 [1] RSPM (R 4.0.2) ## cli 2.0.2 2020-02-28 [1] RSPM (R 4.0.0) ## crayon 1.3.4 2017-09-16 [1] RSPM (R 4.0.0) ## desc 1.2.0 2018-05-01 [1] RSPM (R 4.0.3) ## devtools 2.3.2 2020-09-18 [1] RSPM (R 4.0.3) ## digest 0.6.25 2020-02-23 [1] RSPM (R 4.0.0) ## ellipsis 0.3.1 2020-05-15 [1] RSPM (R 4.0.3) ## evaluate 0.14 2019-05-28 [1] RSPM (R 4.0.3) ## fansi 0.4.1 2020-01-08 [1] RSPM (R 4.0.0) ## fs 1.5.0 2020-07-31 [1] RSPM (R 4.0.3) ## glue 1.6.1 2022-01-22 [1] CRAN (R 4.0.2) ## htmltools 0.5.0 2020-06-16 [1] RSPM (R 4.0.1) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.0.2) ## knitr 1.33 2022-02-15 [1] Github (yihui/knitr@a1052d1) ## lifecycle 1.0.0 2021-02-15 [1] CRAN (R 4.0.2) ## magrittr 2.0.2 2022-01-26 [1] CRAN (R 4.0.2) ## memoise 1.1.0 2017-04-21 [1] RSPM (R 4.0.0) ## pkgbuild 1.1.0 2020-07-13 [1] RSPM (R 4.0.2) ## pkgload 1.1.0 2020-05-29 [1] RSPM (R 4.0.3) ## prettyunits 1.1.1 2020-01-24 [1] RSPM (R 4.0.3) ## processx 3.4.4 2020-09-03 [1] RSPM (R 4.0.2) ## ps 1.3.4 2020-08-11 [1] RSPM (R 4.0.2) ## purrr 0.3.4 2020-04-17 [1] RSPM (R 4.0.3) ## R6 2.4.1 2019-11-12 [1] RSPM (R 4.0.0) ## remotes 2.2.0 2020-07-21 [1] RSPM (R 4.0.3) ## rlang 0.4.10 2022-02-15 [1] Github (r-lib/rlang@f0c9be5) ## rmarkdown 2.10 2022-02-15 [1] Github (rstudio/rmarkdown@02d3c25) ## rprojroot 2.0.2 2020-11-15 [1] CRAN (R 4.0.2) ## sessioninfo 1.1.1 2018-11-05 [1] RSPM (R 4.0.3) ## stringi 1.5.3 2020-09-09 [1] RSPM (R 4.0.3) ## stringr 1.4.0 2019-02-10 [1] RSPM (R 4.0.3) ## testthat 3.0.1 2022-02-15 [1] Github (R-lib/testthat@e99155a) ## usethis 2.1.5.9000 2022-02-15 [1] Github (r-lib/usethis@57b109a) ## withr 2.3.0 2020-09-22 [1] RSPM (R 4.0.2) ## xfun 0.26 2022-02-15 [1] Github (yihui/xfun@74c2a66) ## yaml 2.2.1 2020-02-01 [1] RSPM (R 4.0.3) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
