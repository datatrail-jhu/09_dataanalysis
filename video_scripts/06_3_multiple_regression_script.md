We just finished discussing how to use the l m function to assess the association between two numeric variables. However, there is one incredibly important topic that we have to discuss before moving on from linear regression. 

Confounding in something to watch out for in any analysis you're doing that looks at the relationship between two more more variables. So...what is confounding? Well, let's consider an example. What if we were interested in understanding the relationship between shoe size and literacy. To do so, we took a look at this small sample of two humans, one who wears small shoes and is not literate and one adult who wears big shoes and is literate.

If we were to diagram this question, we may ask "Can we infer literacy rates from shoe size?"

If we return to our sample, it'd be important to note that one of the humans is a young child and the other is an adult. 

Our initial diagram failed to take into consideration the fact that these humans differed in their age. Age affects their shoe size and their literacy rates. In this example, age is a confounder.

Any time you have a variable that affects both your dependent and independent variables, it's a confounder. Ignoring confounders is not appropriate when analyzing data. In fact, in this example, you would have concluded that people who wear small shoes have lower literacy rates than those who wear large shoes. That would have been incorrect. In fact, that analysis was confounded by age. Failing to correct for confounding has led to misreporting in the media and retraction of scientific studies. You don't want to be in that situation. So, always consider and check for confounding among the variables in your dataset.

There are ways to effectively handle confounders within an analysis. Confounders can be included in your linear regression model. When included, the analysis takes into account the fact that these variables are confounders and carries out the regression, removing the effect of the confounding variable from the estimates calculated for the variable of interest. This type of analysis is known as multiple linear regression. As a simple example, let's return to the m t cars dataset, which we've worked with before. In this dataset, we have data from 32 automobiles, including their weight (wt), miles per gallon (mpg), and Engine (vs, where 0 is "V-shaped" and 1 is "straight"). Suppose we were interested in inferring the mpg a car would get based on its weight. We'd first look at the relationship graphically. From the scatterplot, the relationship looks approximately linear and the variance looks constant. 

Thus, we could model this using the linear regression you see here. From this analysis, we would infer that for every increase 1000 pounds more a car weighs, it gets 5.34 miles less per gallon.  

However, we know that the weight of a car doesn't necessarily tell the whole story. The type of engine in the car likely affects both the weight of the car and the miles per gallon the car gets. Graphically, we could see if this were the case by looking at these scatterplots. From this plot, we can see that V-shaped engines tend to be heavier and get fewer miles per gallon while straight engines tend to weigh less and get more miles per gallon. Importantly, however, we see that a car that weighs 3000 points and has a V-Shaped engine gets fewer miles per gallon than a car of the same weight with a straight engine, suggesting that simply modeling a linear relationship between weight and mpg is not appropriate.

Let's then model the data, taking this confounding into account.

Here, we get a more accurate picture of what's going on. Interpreting multiple regression models is slightly more complicated since there are more variables; however, we'll practice how to do so now. The best way to interpret the coefficients in a multiple linear regression model is to focus on a single variable of interest and hold all other variables constant. For instance, we'll focus on weight while holding engine constant to make interpretation easier. This means that for a V-shaped engine, we expect to see a 4 point 4 4  miles per gallon decrease for every 1000 pound increase in weight. 

We can similarly interpret the coefficients by focusing on the engines. For example, for two cars that weigh the same, we'd expect a straight engine to get 3 point 5 more miles per gallon than a V-Shaped engine. Finally, we'll point out that the p-value for wt decreased in this model relative to the model where we didn't account for confounding. This is because the model was not initially taking into account the engine difference. Sometimes when confounders are accounted for, your variable of interest will become more significant; however, frequently, the p-value will increase, and that's OK. What's important is that the data are most appropriately modeled.

You've likely heard someone say before that "correlation is not causation," and it's true! In fact, there are whole websites, such as http://www.tylervigen.com/spurious-correlations, dedicated to this concept.  Let's make sure we know exactly what that means before moving on. In the plot you see here, as the divorce rate in Maine decreases, so does per capita consumption of margarine. These two lines are clearly correlated; however, there isn't really a strong (or any) argument to say that one caused the other. Thus, just because you see two things with the same trend does not mean that one caused the other. These are simply spurious correlations -- things that trend together by chance. Always keep this in mind when you're doing inferential analysis, and be sure that you never draw causal claims when all you have are associations. In fact, one could argue that the only time you can make causal claims are when you have carried out a randomized experiment. Randomized experiments are studies that are designed and carried out by randomly assigning certain subjects to one treatment and the rest of the individuals to another treatment. The treatment is then applied and the results are then analyzed. In the case of a randomized experiment, causal claims can start to be made. Short of this, however, be careful with the language you choose and do not overstate your findings.

To get started on inferential data analysis, let's consider the case where you're interested in analyzing data about a single numeric variable. If you were doing descriptive statistics on this dataset, you'd likely calculate the mean for that variable. But, what if, in addition to knowing the mean, you wanted to know if the values in that variable were all within the bounds of normal variation. You could calculate that using inferential data analysis. You could use the data you have to infer whether or not the data are within the expected bounds. For example, let's say you had a dataset that included the number of ounces actually included in 100 cans of a soft drink. You'd expect that each can have exactly 12 ounces of liquid; however, there is some variation in the process. So, let's test whether or not you're consistently getting shorted on the amount of liquid in your can. In fact, let's go ahead and generate the dataset ourselves using the code you see here! In this code, we're specifying that we want to take a random draw of 100 different values (representing our 100 cans of soft drink), where the mean is 12 (representing the 12 ounces of soda expected to be within each can), and allowing for some variation (we've set the standard deviation to be 0 point 04). We can see that the values are approximately, but not always exactly equal to the expected 12 ounces. 

To make an inference as to whether or not we're consistently getting shorted, we're going to use this sample of 100 cans. Note that we're using this sample of cans to infer something about all cans of this soft drink, since we aren't able to measure the number of ounces in all cans of the soft drink generated. To carry out this statistical test, we'll use a t-test. However, before we can do so we have to ensure that the data follow a normal distribution, since this is the primary assumption of the t-test. Here, we see that the data are approximately normally distributed, allowing for a t-test to be used.

A t-test will check whether the observed ounces differs from the expected mean of 12 ounces. To run a t-test in R, the function is t dot test. In the output from this function, we'll focus on the 95 percent confidence interval. Confidence Intervals provide the range of values likely to contain the unknown population parameter. Here, the population parameter we're interested in is the mean. Thus, the 95 percent Confidence Intervals provides us the range where, upon repeated sampling, the calculated mean would fall 95 percent of the time. More specifically, if the 95 percent confidence interval contains the expected mean, then we can be confident that the company is not shorting us on the amount of liquid they're putting into each can. Here, since 12 is between 11 point 9 9 and 12 point zero zero, we can see that the amounts in the 100 sampled cans are within the expected variation. We could infer from this sample that the population of all cans of this soft drink are likely to have an appropriate amount of liquid in the cans.

However, as mentioned previously, t-tests are an extension of linear regression. We could also look to see whether or not the cans had the expected average of 12 oz in the data collected using the lm function. Note that the confidence interval is the exact same here as above when we used t test function! We bring this up not to confuse you, but to guide you away from trying to memorize each individual statistical test and instead understand how they relate to one another.

Now that you've seen how to measure the linear relationship between variables (linear regression) and how to determine if the mean of a dataset differs from expectation (t-test), it's important to know that you can ask lots of different questions using extensions of linear regression. These have been nicely summarized by  Jonas Kristoffer Lindelov in is blog post Common statistical tests are linear models at https://lindeloev.github.io/tests-as-linear. We've included the table summarizing his post here, but we recommend you check it out and the examples included within it carefully!

In these few lessons on inference, we have covered a lot. We started off discussing that inferential data analysis is required since we often cannot take measurements from everyone in our population of interest. Thus, we take representative samples and estimate what the measure is in the larger population. Since this is an estimate, there is always some measure of uncertainty surrounding this estimate. We talked about how inferences are drawn using a single numeric variable and t-tests. We further discussed how to test whether or not the proportion differs between groups using prop dot test. Finally, we talked about simple and multiple linear regression. We discussed that linear regression helps describe the relationship between two numeric variables and that multiple linear regression helps account for confounding in an analysis. We'll end this lesson by reiterating that being able to draw conclusions and clear interpretations of your analyses is critical and that you should never draw causal conclusions when you have associations.